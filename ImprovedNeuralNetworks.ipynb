{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42fe064-fc00-4d6b-8439-6c3216641500",
   "metadata": {},
   "source": [
    "### Improved neural network training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603dc109-b2dc-41dc-a37c-b7739e618e2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "This code contains the Network class which handles the defining and training of any neural networks we use. In particular this class contains routines such as feed-forward, back-propagation, stochastic gradient descent, network evaluation and more. As mentioned in the script NeuralNetworks.ipynb, the foundations of this neural network code are mostly based on the online tutorial \"Neural Networks and Deep Learning\" by Michael A. Nielsen, however this is the __improved version__ of this code where I've added a lot more functionality including convolutional layers, pooling layers, batch normalisation, dropout and more. \n",
    "\n",
    "To be clear, the parts ot the code which are based on this tutorial were:\n",
    "* the initialisation of weights and biases for networks;\n",
    "* the feedforward and backpropagation routines for fully connected layers;\n",
    "* the organisation of training examples into mini-batches;\n",
    "* evaluation of the network's performance on test data;\n",
    "* the update step for the weights and biases;\n",
    "* implementation of L2 regularisation;\n",
    "* the code for loading MNIST data;\n",
    "\n",
    "The improved parts of the code that I wrote from scratch include:\n",
    "* the initialisation of weights and biases for convolutional layers;\n",
    "* the feedforward and backpropagation routines for convolutional layers;\n",
    "* the feedforward and backpropagation for pooling layers;\n",
    "* the initialisation of parameters 'beta' and 'gamma' for batch normalisation;\n",
    "* the calculation of the global weighted input means and variances used in batch normalisation;\n",
    "* the feedforward and backproagation routines for batch normalised layers;\n",
    "* the update step for batch normalisation parameters beta and gamma;\n",
    "* implementation of dropout regularisation;\n",
    "* batch_backprop routine for running backpropagation on an entire mini-batch of training examples simultaneously;\n",
    "* the initialisation of reinforcement learning hyperparameters;\n",
    "* use of activation functions ReLU and leaky ReLU; \n",
    "* function for visualising convolutional filter weights;\n",
    "* functions for copying networks and editing network exploration parameter/type;\n",
    "\n",
    "#### This code is quite long so feel free to skip straight to Results.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4c99fa1-62eb-4912-a149-0b06b1565b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Importing standard libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "#Miscellaneous functions mostly used in the Network class.\n",
    "#Sigmoid activation function.\n",
    "def sigmoid(z): \n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "#Derivative of the sigmoid function; the chain rule shows that it can be expressed as follows in terms of itself.\n",
    "def sigmoid_prime(z):    \n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "#Rectified linear unit activation function.\n",
    "def ReLU(z):     \n",
    "    return np.maximum(0.0, z)\n",
    "    \n",
    "#Derivative of the rectified linear unit function.\n",
    "def ReLU_prime(z):    \n",
    "    return 0.5*(np.sign(z)+1)\n",
    "\n",
    "#Leaky ReLU function; the gradient in the negative region can be chosen but here I always use a gradient of 0.1 to keep things simple.\n",
    "def leaky_ReLU(z, gradient=1/10):     \n",
    "    return np.maximum(gradient*z, z)\n",
    "        \n",
    "#Derivative of the leaky ReLU function.\n",
    "def leaky_ReLU_prime(z, gradient=1/10):   \n",
    "    #If we assume our derivative is of the form a*(sign(z)+b) then a and b can be solved in terms of the assigned gradient as follows:\n",
    "    b = 2/(1-gradient) - 1\n",
    "    a = 1/(1+b)\n",
    "    return a*(np.sign(z)+b)\n",
    "    \n",
    "#Function that takes an integer j and returns a 10-entry vector with a 1 in the j^th position and 0 everywhere else.\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b248487-610f-4d11-99ba-0f6d37541e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Assorted functions made use of in convolutional layers, pooling layers and batch normalisation.\n",
    "\n",
    "#Here we import some mathematical operations used in the convolutional layers.\n",
    "#These typically run faster than the equivalent scripts I made for performing these operations so to speed things up I use them wherever I can.\n",
    "from scipy.signal import convolve2d, correlate2d  #Importing functions to perform efficent 2D convolution and cross-correlation.\n",
    "from scipy.linalg import block_diag #Importing a function to quickly construct block diagonal matrices.\n",
    "from skimage.measure import block_reduce #Importing a function to perform efficient max-pooling.\n",
    "\n",
    "#Function for calculating the expected output shape of the array resulting from the convolution of our 'input layer' with a filter whose dimensions are given.\n",
    "def convolution_output_shape(input_layer_dimensions, filter_dimensions, padding):\n",
    "    #Whether or not we use padding will effect the number of outputs of our convolution and thus the number of weights our convolutional layer needs. Note that with padding on we will require more weights.\n",
    "    if input_layer_dimensions[0] < filter_dimensions[0]: (input_layer_dimensions, filter_dimensions) = (filter_dimensions, input_layer_dimensions)   #If the filter is larger than the layer then the role of the two are swapped in the convolution operation, hence we swap their roles here too.\n",
    "    if padding == True: output_shape = [input_layer_dimensions[0]+filter_dimensions[0]-1, input_layer_dimensions[1]+filter_dimensions[1]-1] \n",
    "    if padding == False: output_shape = [input_layer_dimensions[0]-filter_dimensions[0]+1, input_layer_dimensions[1]-filter_dimensions[1]+1]\n",
    "    return output_shape\n",
    "\n",
    "#Function for determining the amount of padding needed in our max-pooling process.\n",
    "def pooling_padding_needed(input_layer_dimensions, pooling_filter_dimensions, stride):\n",
    "    padding_dimensions = [0,0]\n",
    "    for i in [0, 1]:\n",
    "        padding_dimensions[i] = stride*np.ceil((input_layer_dimensions[i]-pooling_filter_dimensions[i])/stride) - input_layer_dimensions[i] + pooling_filter_dimensions[i]\n",
    "    return padding_dimensions\n",
    "\n",
    "#Function for performing max-pooling on a given array.\n",
    "def max_pooling(input_array, pooling_size, stride, return_indices = False):\n",
    "    #If we have a stride which doesn't match our pooling size or if we want to keep track of each maximal index during our pooling step we can't simply use the block_reduce function so we need to max pool manually.\n",
    "    input_array_dimensions = np.shape(input_array)  #Finding the shape of the input array.\n",
    "    padding_dimensions = pooling_padding_needed(input_array_dimensions, (pooling_size, pooling_size), stride) #Calculating the amount of padding we will need.\n",
    "    if return_indices == True or stride != pooling_size:\n",
    "        #Calculating the array size we will need for our pooled output.\n",
    "        output_shape = max_pooling_output_shape(input_array_dimensions, pooling_size, stride)\n",
    "        #Creating the padded array and inserting our input_array into the top left corner.\n",
    "        padded_array = np.zeros((int(input_array_dimensions[0]+padding_dimensions[0]), int(input_array_dimensions[1]+padding_dimensions[1])))\n",
    "        padded_array[0:input_array_dimensions[0], 0:input_array_dimensions[1]] = input_array\n",
    "        #Performing the max-pooling process.\n",
    "        output_array = np.zeros(output_shape)\n",
    "        indices = []\n",
    "        for i in range(output_shape[0]):  #Looping over y indices. \n",
    "            for j in range(output_shape[1]):   #Looping over x indices. \n",
    "                #Defining the current window that our pooling algorithm is going to max-pool over.\n",
    "                pooling_window = padded_array[i*stride:i*stride+pooling_size, j*stride:j*stride+pooling_size]\n",
    "                max_index = pooling_window.argmax()\n",
    "                #Calculating the current row and column of the maximal index in our pooling window.\n",
    "                row = max_index//pooling_size\n",
    "                column = max_index%pooling_size\n",
    "                max_value = pooling_window[row, column]\n",
    "                output_array[i, j] = max_value\n",
    "                if max_value == 0: indices.append(None)   #We don't append any values at padded indices (these will always be zero).\n",
    "                else: indices.append([i*stride+row, j*stride+column]) #Here we save the original indices of the entries that survives max-pooling.\n",
    "        return output_array, indices\n",
    "    else:  #In the special case that our stride matches our pooling size we can use the block_reduce function from skimage which typically runs faster.\n",
    "        output_array = block_reduce(input_array, (pooling_size, pooling_size), np.max)\n",
    "        return output_array\n",
    "    \n",
    "#Equation for calculating the expected output shape of an array with a given input shape after max-pooling using a certain pooling size and stride.\n",
    "def max_pooling_output_shape(input_array_dimensions, pooling_size, stride):\n",
    "    padding_dimensions = pooling_padding_needed(input_array_dimensions, (pooling_size, pooling_size), stride)\n",
    "    output_shape = (int((input_array_dimensions[0]+padding_dimensions[0]-pooling_size+stride)/stride), int((input_array_dimensions[1]+padding_dimensions[1]-pooling_size+stride)/stride))\n",
    "    return output_shape\n",
    "\n",
    "#Code for batch normalising a single layer of weighted inputs mb_z from a mini-batch of size m.\n",
    "def batch_normalise(mb_z, eps, gamma, beta, m): \n",
    "    #Reshaping the mini-batch weighted inputs to allow us to easily calculate the mean and variance over the mini-batch.\n",
    "    mb_z = np.reshape(mb_z, (m, np.size(gamma))).T\n",
    "    #Calculating the mean and variance of the weighted inputs.\n",
    "    means = np.mean(mb_z, 1, keepdims=True)\n",
    "    variances = np.var(mb_z, 1, keepdims=True)\n",
    "    #Using the mini-batch mean and variance we normalise the weighted inputs.\n",
    "    normalised_mb_z = (mb_z-means)/np.sqrt(variances+eps)\n",
    "    normalised_mb_z = np.reshape(normalised_mb_z.T, (np.size(normalised_mb_z), 1))\n",
    "    #Finally we shift and scale these weighted inputs using our network's learned gamma and beta parameters.\n",
    "    mb_gamma = np.tile(gamma, (m,1)) \n",
    "    mb_beta = np.tile(beta, (m,1))\n",
    "    final_mb_z = mb_gamma*normalised_mb_z+mb_beta\n",
    "    return final_mb_z, normalised_mb_z, means, variances  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689b448-924c-498e-856d-9982037e1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for importing data sets.\n",
    "import pickle  \n",
    "import gzip\n",
    "\n",
    "#Code for loading the MNIST data into the python runtime. \n",
    "def load_data():\n",
    "    #Opening the file containing the MNIST data.\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    #Loading the 50,000 training examples and the 10,000 validation and test examples.\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    return (training_data, test_data) #Here I don't use validation data and just return the training and test data.\n",
    "\n",
    "#Code for organising the training and test data so that its ready for training use.\n",
    "def load_data_wrapper():\n",
    "    #Loading in the MNIST training data and test data using our load_data() routine above.\n",
    "    tr_d, te_d = load_data()\n",
    "    #Organising the 50,000 training data images from MNIST with their corresponding labels.\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    #For the test data we save the labels in vector form rather than as integers.\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]] \n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    #Organising the 10,000 test data images from MNIST with their corresponding labels.\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670c602-473a-4526-b50f-3bbab8badad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network class: this is the heart of the code as it contains all functions used in defining and training the neural networks.\n",
    "#This class contains routines such as feed-forward, back-propagation, stochastic gradient descent, network evaluation and more.\n",
    "#The inputs to this class are the hyperparameters for the neural network which are each explained inside the __init__ routine.\n",
    "#The `layers' input is simple for fully connected networks; the entries of layers denote the number of nodes you want in each layer.\n",
    "#For convolutional networks, 'layers' becomes more complicated as more values need to be prescribed so this is best understood by looking at examples in the results.\n",
    "class Network(object):\n",
    "    def __init__(self, layers, activation_function = sigmoid, padding = False, dropout_frequency = 0, batch_normalisation = False, data_augmentation = True, game = None, exploration_type = \"epsilon-greedy\", exploration_parameter = 0):\n",
    "        \n",
    "        #Initialising general neural network hyperparameters:\n",
    "        self.num_layers = len(layers) #Number of layers.\n",
    "        self.layers = layers #Data used for describing each layer.\n",
    "        self.padding = padding #Here we set a flag saying whether or not to use zero-padding in any convolutional steps. \n",
    "        self.activation_function = activation_function #Activation functions can be sigmoid, ReLU or leaky_ReLU.\n",
    "        #Assigning the derivatives of the chosen activation functions.\n",
    "        if activation_function == ReLU: self.activation_function_prime = ReLU_prime\n",
    "        if activation_function == sigmoid: self.activation_function_prime = sigmoid_prime\n",
    "        if activation_function == leaky_ReLU: self.activation_function_prime = leaky_ReLU_prime\n",
    "        self.batch_normalisation = batch_normalisation #Flag saying whether or not to use batch normalisation.\n",
    "        self.dropout_frequency = dropout_frequency #The rate at which nodes are dropped in all layers (except the input layer).\n",
    "        \n",
    "        #Initialising reinforcement learning exclusive hyperparameters:\n",
    "        self.game = game #The game to be trained on can be \"connect 4\" or \"noughts and crosses\".\n",
    "        self.exploration_type = exploration_type #Exploration types can be \"epsilon-greedy\", \"softmax\" or \"adaptive-greedy\".\n",
    "        self.exploration_parameter = exploration_parameter #Exploration parameter; for \"epsilon-greedy this represents epsilon, otherwise this represents the exploration temperature T.\n",
    "        self.data_augmentation = data_augmentation #Whether or not data augmentation is used to generate new experiences by flipping/rotating boards.\n",
    "        #Creating an array for storing a network's experiences in reinforcement learning.\n",
    "        self.experiences = []\n",
    "        \n",
    "        #Initialising storage for our weights and biases.\n",
    "        self.weights = [None for layer in layers[:-1]]  \n",
    "        self.biases = [None for layer in layers[1:]]\n",
    "        \n",
    "        #Batch normalisation initialisations.\n",
    "        if batch_normalisation == True:\n",
    "            #Initialising storage for our gamma and beta variables.\n",
    "            self.gammas = [None for layer in layers[:-2]] \n",
    "            self.betas = [None for layer in layers[:-2]] \n",
    "            self.epsilon = 1e-4 #Value of epsilon added to the variance in the division part of the batch normalisation step (not to be confused with epsilon in epsilon-greedy exploration which is saved under `exploration_parameter').\n",
    "            #Setting default values of the global mean and the inverse global variance used for feedforward in evaluation and gameplay (these are updated during learning).\n",
    "            self.global_mean = 0 \n",
    "            self.inv_sqrt_global_variance = 1 \n",
    "            \n",
    "        #Here we don't allow our final layer to be convolutional as it needs to have a node for every choice. Typically this isn't a strict requirement for a network but I'm enforcing it here to make things slightly simpler\n",
    "        if type(layers[-1]) == list:\n",
    "            print(\"Final layer must not be a convolutional layer.\")\n",
    "            return\n",
    "        \n",
    "        #Looping over each layer in our network to initialise weights and biases in each layer.\n",
    "        for i in range(self.num_layers-1):\n",
    "            \n",
    "            #Initialising parameters for convolutional layers.\n",
    "            #We can describe a convolutional layer in our network by entering a 6 component list entry in place of the usual 'layer_size' entry in our layers array.\n",
    "            #The 6 components are: [input_layer_rows, input_layer_columns, num_filters, filter_size (filters are assumed to be square), pooling_size, pooling_stride]\n",
    "            #Hence we need to describe the shape that our input activations should be rearranged into using input_layer_rows and input_layer_columns.\n",
    "            if type(layers[i]) == list:   \n",
    "                #Here we make sure that the correct data is given to describe our convolutional layer.\n",
    "                if len(layers[i]) < 4:   \n",
    "                    print(\"Convolutional layer is missing required arguments: input_layer_rows, input_layer_columns, num_filters, filter_size. Optional arguments also include pooling_size, pooling_stride (with default values 1).\")\n",
    "                    return\n",
    "                if len(layers[i]) > 6:\n",
    "                    print(\"Too many input arguments for convolutional layer. Allowed input arguments are: input_layer_rows, input_layer_columns, num_filters, filter_size, pooling_size, pooling_stride.\")\n",
    "                    return\n",
    "                #If we don't specify a pooling size or a pooling stride then we set these to have value 1 by default.\n",
    "                while len(layers[i]) < 6:  \n",
    "                    layers[i].append(1)\n",
    "                    \n",
    "                #Unpacking our convolutional layer information.\n",
    "                [input_layer_rows, input_layer_columns, num_filters, filter_size, pooling_size, pooling_stride] = layers[i]  \n",
    "                input_layer_size = input_layer_rows*input_layer_columns\n",
    "                #In the convolutional layer we need a bias for every node in the layer and we also need filter_size weights per filter as well as a next layer bias for every filter.\n",
    "                #Our weights are randomly initialised using a normal distribution with mean 0 and s.d. given by 1/sqrt(no. of input weights to node) for each node.\n",
    "                #This means that our weighted input 'z' into the node is much more likely to be a small number (around -1 to 1 usually) which avoids large activation values and reduces node saturation.\n",
    "                self.biases[i] = np.array(np.random.randn(num_filters, 1),dtype=np.float32) #Here np.random.randn(x, 1) makes a x-by-1 array of random numbers.\n",
    "                self.weights[i] = np.array(np.random.randn(num_filters, filter_size*filter_size)/np.sqrt(input_layer_size),dtype=np.float32)  #Here we normalise by our filter size in each filter.\n",
    "                #We need to ensure that the output of our convolutional layer fits the input of the next layer. Note that whether or not we use padding will effect the number of outputs of our convolutional layer.\n",
    "                layer_dimensions, filter_dimensions = [input_layer_rows, input_layer_columns], [filter_size, filter_size]\n",
    "                #Determining the size we will need for the layer following our convolutional layer. \n",
    "                conv_layer_output_size = convolution_output_shape(layer_dimensions, filter_dimensions, padding)\n",
    "                if pooling_size > 1:  #If we are adding pooling after our convolution then the size of the output will also depend on the pooling size and stride.\n",
    "                    for j in [0, 1]:\n",
    "                        conv_layer_output_size[j] = 1 + np.ceil((conv_layer_output_size[j]-pooling_size)/pooling_stride)\n",
    "                conv_layer_output_size = int(num_filters*conv_layer_output_size[0]*conv_layer_output_size[1])\n",
    "                #Checking to see whether the current layer is compatible with the output of the previous convolutional layer.\n",
    "                if type(layers[i+1]) == list: next_layer_input_size = layers[i+1][0]*layers[i+1][1]\n",
    "                else: next_layer_input_size = layers[i+1]\n",
    "                #If the output of a convolutional layer doesn't match the input of the next layer then we print out an error message and return.\n",
    "                if conv_layer_output_size != next_layer_input_size:\n",
    "                    print(\"A layer after a convolutional layer doesn't match the size of the expected convolutional layer output: \",next_layer_input_size,\"!=\",conv_layer_output_size)\n",
    "                    return\n",
    "            \n",
    "            #Initialising parameters for fully connected layers.\n",
    "            else:\n",
    "                if type(layers[i+1]) == list: \n",
    "                    next_layer_size = layers[i+1][0]*layers[i+1][1]  #Determining the size needed for inputs to our layer if it's a convolutional layer.                    \n",
    "                else: next_layer_size = layers[i+1]  #Otherwise we have a fully connected layer so its size is simply prescribed by the value of layers[i+1].\n",
    "                self.biases[i] = np.array(np.random.randn(next_layer_size, 1),dtype=np.float32) #Note that we don't define biases for fully connected first layer nodes.\n",
    "                self.weights[i] = np.array(np.random.randn(next_layer_size, layers[i])/np.sqrt(layers[i]),dtype=np.float32) \n",
    "                #Initialising batch_normalisation parameters.\n",
    "                if batch_normalisation == True and i != self.num_layers-2: \n",
    "                    self.gammas[i] = np.array(np.random.randn(layers[i], 1),dtype=np.float32) #Randomly initialising gamma values for all layers but our last two layers.\n",
    "                    self.betas[i] = np.array(np.random.randn(layers[i], 1),dtype=np.float32) #Randomly initialising gamma values for all layers but our last two layers.\n",
    "\n",
    "    #Code for feeding forward an array of input activations 'a' through our network to produce output activations.\n",
    "    def feedforward(self, a):\n",
    "        #Calculations for batch normalisation of our input activations (if batch_normalisation is enabled).\n",
    "        if self.batch_normalisation == True:\n",
    "            gamma = self.gammas[0]\n",
    "            beta = self.betas[0]\n",
    "            if self.global_mean is 0: #If we are feeding forward without having done any training we don't have any global means/variances, hence a is already normalised.\n",
    "                normalised_a = a \n",
    "            else: #Otherwise we normalise our input activations using the saved global mean and global variance from all examples.\n",
    "                normalised_a = (a-self.global_mean[0])*self.inv_sqrt_global_variance[0]\n",
    "            #Scaling and shifting the normalised activations using the learned parameters gamma and beta.\n",
    "            a = gamma*normalised_a+beta  \n",
    "            \n",
    "        #Here we loop over each layer in our network.\n",
    "        layer = 0 #Current layer counter.\n",
    "        for b, w in zip(self.biases, self.weights):   #The arrays of both the bias vectors and weight matrices have length num_layers-1 since there are no first layer biases and no final layer weights.\n",
    "           \n",
    "            #Convolutional layer calculations.\n",
    "            if type(self.layers[layer]) == list:   #If the current layer is described by a list then it is a convolutional layer so we perform a convolution to get the activation.\n",
    "                [input_layer_rows, input_layer_columns, num_filters, filter_size, pooling_size, pooling_stride] = self.layers[layer]  #Unpacking our convolutional layer information.\n",
    "                 #We convolve (or technically cross-correlate) each of our filters with 2D array of activations fed into our convolutional layer by our previous layer to get a weighted input for each filter.\n",
    "                reshaped_a = np.reshape(a, (input_layer_rows, input_layer_columns))\n",
    "                a = np.array((),dtype=np.float32)\n",
    "                #Calculating convolutions using each different filter.\n",
    "                for j in range(num_filters):  #The weights for each filter are stored in separate vectors contained in the entry of 'weights' corresponding to the convolutional layer.\n",
    "                    convolution_filter = np.reshape(w[j], (filter_size, filter_size))\n",
    "                    #If we have padding enabled then we obtain an outputs that include the cases where our filter isn't entirely overlapping with our 2D array of nodes, i.e. when it's hanging off the edge.\n",
    "                    if self.padding == True: z = correlate2d(reshaped_a, convolution_filter, 'full') + b[j]  \n",
    "                    #Otherwise we only obtain outputs from calculations made when our filter exactly fits onto our 2D array of nodes.\n",
    "                    if self.padding == False: z = correlate2d(reshaped_a, convolution_filter, 'valid') + b[j] \n",
    "                     #If a pooling filter size is prescribed we then apply max-pooling to the output of the convolution of our filter with our input activations.\n",
    "                    if pooling_size > 1: z = max_pooling(z, pooling_size, pooling_stride)\n",
    "                    #Finally we apply our activation function to the pooled output and append this to our array of activations. \n",
    "                    a = np.append(a, self.activation_function(z))   \n",
    "                a = a.flatten()[:, np.newaxis]\n",
    "            \n",
    "            #Fully connected layer calculations.\n",
    "            else:\n",
    "                #Batch normalisation step: here we need to normalise our weighted inputs for each layer and scale them using gamma and beta. \n",
    "                #During the testing phase we normalise using the mean and standard deviation from the whole training set (rather than from a mini-batch).\n",
    "                if self.batch_normalisation == True and layer < self.num_layers-3:\n",
    "                    z = w@a\n",
    "                    gamma = self.gammas[layer+1]\n",
    "                    beta = self.betas[layer+1]\n",
    "                    if self.global_mean is 0: normalised_z = z #Accounting for when we are feeding forward without having done any training (so we don't have any global means/variances).\n",
    "                    else: normalised_z = (z-self.global_mean[layer+1])*self.inv_sqrt_global_variance[layer+1]\n",
    "                    z = gamma*normalised_z+beta \n",
    "                else: #Fully connected layer calculations.\n",
    "                    z = w@a+b\n",
    "                #If we are training our network to play games then we always use a linear final layer as estimating action-values is a regression problem.\n",
    "                if layer == self.num_layers-2 and self.game != None: a = z\n",
    "                #Otherwise we apply our activation function to each weighted input to introduce non-linearity into our network.\n",
    "                else: a = self.activation_function(z)                            \n",
    "            layer = layer + 1  #Incrementing our layer counter.\n",
    "        return a\n",
    "\n",
    "    \n",
    "    #Function for training a neural network using stochastic gradient descent for a given number of epochs.\n",
    "    #Here eta is the learning rate and lmbda is the hyperparameter for L2 regularisation.\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data = None, lmbda = 0):\n",
    "        \n",
    "        #Checking that we have an appropriate mini-batch size if batch normalisation is being used.\n",
    "        if self.batch_normalisation == True and mini_batch_size == 1:\n",
    "            print(\"In order for batch normalisation to be used, mini_batch_size must be greater than 1.\")\n",
    "            return\n",
    "        #Recording the size of the test and training data and performing an initial evaluation on the untrained network.\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "            correct = self.evaluate(test_data)\n",
    "            accuracies = [100*correct/n_test]\n",
    "        n = len(training_data) \n",
    "        \n",
    "        #Looping over each epoch.\n",
    "        for j in range(epochs):\n",
    "            #Creating the mini-batches.\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            global_mean, global_variance = 0, 0\n",
    "            for mini_batch in mini_batches:\n",
    "                #Performing a single gradient descent step for each mini-batch.\n",
    "                mb_means, mb_variances = self.mini_batch_gradient_descent(mini_batch, eta, lmbda, n) #If we aren't using batch normalisation then means and variances will return as None.\n",
    "                if self.batch_normalisation == True:\n",
    "                    global_mean = np.add(global_mean, mb_means)\n",
    "                    global_variance = np.add(global_variance, mb_variances)\n",
    "            #If we are using batch normalisation then we need to calculate the mean and variance of our entire training set for use in the testing phase later.\n",
    "            #We calculate the population mean/variance using averages from all of the mini-batch means/averages calculated during training.\n",
    "            if self.batch_normalisation == True:\n",
    "                inv_len_mini_batches = 1/len(mini_batches)\n",
    "                self.global_mean = global_mean*inv_len_mini_batches\n",
    "                global_variance = mini_batch_size/(mini_batch_size-1)*global_variance*inv_len_mini_batches\n",
    "                if np.size(global_variance) > 1: self.inv_sqrt_global_variance = [1/np.sqrt(gv+self.epsilon) for gv in global_variance]\n",
    "            #If test data is provided then the network will evaluate its performance against this data after every epoch.\n",
    "            if test_data:\n",
    "                correct = self.evaluate(test_data)\n",
    "                accuracies.append(100*correct/n_test)\n",
    "        if test_data: return accuracies\n",
    "    \n",
    "    #This code tweaks the network's learnable parameters using gradient descent over a single mini-batch.\n",
    "    def mini_batch_gradient_descent(self, mini_batch, eta, lmbda, n):\n",
    "        #Initialising storage for the gradients of our cost function C wrt the network's weights and biases.\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #Defining a coefficient used in our weight update step\n",
    "        w_coeff = 1-eta*(lmbda/n)\n",
    "        inv_len_mini_batch = 1/len(mini_batch)\n",
    "        #All our gradient terms below (e.g. grad_w, grad_b) use the below coefficient in our update step.\n",
    "        grad_coeff = eta*inv_len_mini_batch \n",
    "        convolutional = False\n",
    "        for layer in self.layers:         #Determining whether our network has any convolutional layers.\n",
    "            if type(layer) == list: convolutional = True\n",
    "           \n",
    "        #Calculations without batch normalisation.\n",
    "        if self.batch_normalisation == False: \n",
    "            #We only use batch backpropagation if our mini-batch size is greater than 1 and if our network has no convolutional layers.\n",
    "            if convolutional == False and len(mini_batch) > 1: grad_b, grad_w = self.batch_backprop(mini_batch) \n",
    "            else: \n",
    "                for x, y in mini_batch:\n",
    "                    delta_grad_b, delta_grad_w = self.backprop(x, y)\n",
    "                    grad_b = [gb+dgb for gb, dgb in zip(grad_b, delta_grad_b)]\n",
    "                    grad_w = [gw+dgw for gw, dgw in zip(grad_w, delta_grad_w)]\n",
    "            #Update step for our learnable parameters.\n",
    "            self.weights = [w_coeff*w-grad_coeff*gw for w, gw in zip(self.weights, grad_w)]\n",
    "            self.biases = [b-grad_coeff*gb for b, gb in zip(self.biases, grad_b)]  \n",
    "            return None, None\n",
    "        \n",
    "        #Batch normalisation calculations.\n",
    "        if self.batch_normalisation == True: \n",
    "            grad_gamma, grad_beta, grad_b, grad_w, mb_means, mb_variances = self.batch_backprop(mini_batch)\n",
    "            #Update step for our learnable parameters.\n",
    "            self.biases = [b-grad_coeff*gb for b, gb in zip(self.biases, grad_b)]  \n",
    "            self.weights = [w_coeff*w-grad_coeff*gw for w, gw in zip(self.weights, grad_w)]\n",
    "            self.gammas = [gamma-grad_coeff*ggamma for gamma, ggamma in zip(self.gammas, grad_gamma)]\n",
    "            self.betas = [beta-grad_coeff*gbeta for beta, gbeta in zip(self.betas, grad_beta)]\n",
    "            return mb_means, mb_variances\n",
    "\n",
    "    #Function for back-propagation through our network taking in one mini batch tuple input (x,y) at a time to calculate the gradient\n",
    "    #vectors grad_b and grad_w of our cost function C wrt our weights and biases.\n",
    "    def backprop(self, x, y): \n",
    "        #Initialising arrays to store our partial derivatives of C wrt weights and biases.\n",
    "        grad_b = [np.array(np.zeros(np.shape(b)),dtype=np.float32) for b in self.biases] \n",
    "        grad_w = [np.array(np.zeros(np.shape(w)),dtype=np.float32) for w in self.weights] \n",
    "        #Here we feedforward to calculate our weighted inputs z^l and our activations a^l := activation_function(z) for each layer l.\n",
    "        #Using these we can then calculate the final layer error vector delta^L and the other layer error vectors delta^l for all l.\n",
    "        activation = x #This sets our mini batch inputs as our first layer activation vector.\n",
    "        activations = [activation] #This creates a matrix to store each of our layer activations.\n",
    "        zs = [] #This creates an array to store all of our weighted input vectors z (or z^l).\n",
    "        max_pooling_indices = [] #This creates an array to keep track of any maximal indices used in any max pooling steps we perform (we need these later for backpropagtion).\n",
    "        \n",
    "        #Dropout calculations\n",
    "        if self.dropout_frequency > 0: \n",
    "            #If we are using dropout then we create a 'mask' for each hidden layer which tell us which neurons to drop in each layer (this will vary per training example).\n",
    "            masks = [] #Creating an array to hold the mask for each layer.\n",
    "            for l in range(self.num_layers-3): #In this case we don't perform dropout on the first (input) layer, final (output) layer or the penultimate layer so we only create (up to) num_layers-3 masks.\n",
    "                if type(self.layers[l+1]) == list: masks.append(None) #We won't perform dropout on any convolutional layers. \n",
    "                else: masks.append(np.random.binomial(1, 1-self.dropout_frequency, (self.layers[l+1], 1))/(1-self.dropout_frequency)) #We divide by the dropout frequency since we are using 'inverted dropout'.       \n",
    "        \n",
    "        #Feed-forward part of our backpropagation routine.\n",
    "        layer = 0\n",
    "        for b, w in zip(self.biases, self.weights):   #The arrays of both the bias vectors and weight matrices have length num_layers-1 since there are no first layer biases and no final layer weights.\n",
    "            max_z_indices = []\n",
    "            \n",
    "            #Convolutional layer calculations.\n",
    "            if type(self.layers[layer]) == list:   #If the current layer is described by a list then it is a convolutional layer so we perform a convolution to get the activation.\n",
    "                [input_layer_rows, input_layer_columns, num_filters, filter_size, pooling_size, pooling_stride] = self.layers[layer]  #Unpacking our convolutional layer information.\n",
    "                 #We convolve (or technically cross-correlate) each of our filters with 2D array of activations fed into our convolutional layer by our previous layer to get a weighted input for each filter.\n",
    "                reshaped_activation = np.reshape(activation, (input_layer_rows, input_layer_columns))\n",
    "                z = np.array((),dtype=np.float32)\n",
    "                activation = np.array((),dtype=np.float32)\n",
    "                #Calculating the convolution for each filter.\n",
    "                for f in range(num_filters):  #The weights for each filter are stored in separate vectors contained in the entry of 'weights' corresponding to the convolutional layer.\n",
    "                    convolution_filter = np.reshape(w[f], (filter_size, filter_size))  \n",
    "                    if self.padding == True: z_f = correlate2d(reshaped_activation, convolution_filter, 'full') + b[f]  #If we have padding enabled then we obtain an outputs that include the cases where our filter isn't entirely overlapping with our 2D array of nodes, i.e. when it's hanging off the edge.\n",
    "                    if self.padding == False: z_f = correlate2d(reshaped_activation, convolution_filter, 'valid') + b[f] #Otherwise we only obtain outputs from calculations made when our filter exactly fits onto our 2D array of nodes.\n",
    "                    #Max-pooling calculations.\n",
    "                    if pooling_size > 1: \n",
    "                        z_f, max_z_f_indices = max_pooling(z_f, pooling_size, pooling_stride, True) #If a pooling filter size is prescribed we then apply max-pooling to the output of the convolution of our filter with our input activations.\n",
    "                        max_z_indices.append(max_z_f_indices)\n",
    "                    z = np.append(z, z_f)  #Appending the weighted input from this filter to our overall weighted input.\n",
    "                    activation = np.append(activation, self.activation_function(z_f))   #We then apply our activation function to the output obtained by convolving our filter with our input activations and then append this to the array of activations.\n",
    "                z = z.flatten()[:, np.newaxis]  #Reshaping our overall z and activation into the format that works with our backpropagation algorithm.\n",
    "                activation = activation.flatten()[:, np.newaxis]\n",
    "            \n",
    "            #Fully connected layer calculations.\n",
    "            else:\n",
    "                z = w@activation+b   #Calculating our weighted input (for a non-convolutional layer).\n",
    "                #If we are training our network to play games then we always use a linear final layer as estimating action-values is a regression problem.\n",
    "                if layer == self.num_layers-2 and self.game != None: activation = z   \n",
    "                #Here we apply our activation function which introduces non-linearity into our network.\n",
    "                else: activation = self.activation_function(z)  \n",
    "            activations.append(activation)\n",
    "            \n",
    "            #We never apply dropout to our output layer or the penultimate layer. Also the saved activations for backpropagation aren't the dropout activations since the backpropagation dropout is achieved by applying our mask to the delta passed back to the dropout layer.\n",
    "            if self.dropout_frequency > 0 and layer < self.num_layers-3: \n",
    "                if masks[layer] is not None: \n",
    "                    activation = activation * masks[layer] #Applying dropout to hidden layers on the forward pass.\n",
    "            zs.append(z)                    \n",
    "            max_pooling_indices.append(max_z_indices)\n",
    "            layer = layer + 1\n",
    "            \n",
    "        #Backpropagation part of our backprop routine.\n",
    "        #First we calculate the error in the final layer using our output activations and the derivative of our cost function.\n",
    "        if self.game != None: delta = self.cost_derivative(activations[-1], y) #If we are training for reinforcement learning then this \n",
    "        else: delta = self.cost_derivative(activations[-1], y)*self.activation_function_prime(zs[-1]) #Calculates our final layer error vector.\n",
    "\n",
    "        #Next we backpropagate one layer at a time to calculate the error delta for each layer and then we calculate our cost gradient vectors grad_b and grad_w for each layer.\n",
    "        #First we use our last layer delta to calculate the next delta and pass it on: since the last layer is forced to not be convolutional this is just our standard delta calculation.\n",
    "        for l in range(1, self.num_layers): \n",
    "            if l != self.num_layers-1: z = zs[-l-1] #Unless we are on the first layer we will need our previous layer weighted input at each step.\n",
    "            \n",
    "            #Convolutional layer calculations.\n",
    "            if type(self.layers[-l-1]) == list:  \n",
    "                #Using the current layer delta to calculate grad_b and grad_w for a convolutional layer.\n",
    "                [input_layer_rows, input_layer_columns, num_filters, filter_size, pooling_size, pooling_stride] = self.layers[-l-1]  #Unpacking the data for our convolutional layer.\n",
    "                #Calculating the delta to be passed down to the next layer; note that as our current layer is a convolutional layer the calculations are more complicated than before.\n",
    "                layer_dimensions, filter_dimensions = [input_layer_rows, input_layer_columns], [filter_size, filter_size]\n",
    "                delta_f_sizes = convolution_output_shape(layer_dimensions, filter_dimensions, self.padding)  #Calculating the dimension that each delta_f array should have when reshaped.\n",
    "                unpooled_delta_f_sizes = delta_f_sizes\n",
    "                #Pooling calculations.\n",
    "                if pooling_size > 1:   #If pooling was used then the dimensions in delta_f_sizes will be smaller than as calculated above.\n",
    "                    padding_dimensions = pooling_padding_needed(delta_f_sizes, (pooling_size, pooling_size), pooling_stride)\n",
    "                    delta_f_sizes = (int((delta_f_sizes[0]+padding_dimensions[0]-pooling_size+pooling_stride)/pooling_stride), int((delta_f_sizes[1]+padding_dimensions[1]-pooling_size+pooling_stride)/pooling_stride))\n",
    "                delta_f_size = delta_f_sizes[0]*delta_f_sizes[1]\n",
    "                delta_fs = []  #Creating an array to store the delta arrays for each filter.\n",
    "                reshaped_activation = np.reshape(activations[-l-1], (input_layer_rows, input_layer_columns))\n",
    "                for f in range(num_filters):  #For convolutional layers we build up our gradients using the deltas for each filter (i.e. the sections of the overall delta).\n",
    "                    convolution_filter = np.reshape(self.weights[-l][f], (filter_size, filter_size))  \n",
    "                    delta_f = np.reshape(delta[f*delta_f_size:(f+1)*delta_f_size], (delta_f_sizes[0], delta_f_sizes[1]))\n",
    "                    #If pooling has been used then we essentially need to undo our pooling step by increasing the size of delta_f and inserting zeros at the entries of the non-maximal elements.\n",
    "                    if pooling_size > 1:   #Pooling layer delta_f calculations; here we only have non-zero entries of delta_f corresponding to the activations which were maximal in the max pooling step.\n",
    "                        z_f_shape = convolution_output_shape((input_layer_rows, input_layer_columns), (filter_size, filter_size), self.padding)\n",
    "                        unpooled_delta_f = np.zeros(z_f_shape)   #Initialising an array the size of our convolutional layer's convolution output.\n",
    "                        max_indices = max_pooling_indices[-l][f]\n",
    "                        k = 0  #Counter to move through the max_indices array.\n",
    "                        for i in range(delta_f_sizes[0]):    #Looping over rows.\n",
    "                            for j in range(delta_f_sizes[1]):   #Looping over columns.\n",
    "                                if max_indices[k] is not None:\n",
    "                                    unpooled_delta_f[max_indices[k][0],max_indices[k][1]] = delta_f[i,j]\n",
    "                                k = k + 1\n",
    "                        delta_f = unpooled_delta_f\n",
    "                    grad_b[-l][f] = sum(delta_f.flatten())   #Calculating grad_b for the bias associated with filter f.\n",
    "                    grad_w[-l][f] = correlate2d(reshaped_activation, delta_f, 'valid').flatten()   #Calculating grad_w for the weights associated with filter f.\n",
    "                    #We obtain a contribution to the error to be passed to our next layer by convolving the individual errors for each filter (i.e. each delta_f) with that filter.\n",
    "                    if l != self.num_layers-1:\n",
    "                        next_delta_f_sizes = convolution_output_shape(np.shape(convolution_filter), unpooled_delta_f_sizes, True)\n",
    "                        #We use convolution to calculate the next delta to be passed down for each filter.\n",
    "                        delta_f = convolve2d(delta_f, convolution_filter, 'full')*np.reshape(self.activation_function_prime(z), next_delta_f_sizes)\n",
    "                        delta_fs.append(delta_f)\n",
    "                #The overall error to be passed down to the next layer is then the elementwise sum of all of the delta_fs. Note that for our last layer we can skip this step as there is no further layer to pass on a delta to.\n",
    "                if l != self.num_layers-1:   \n",
    "                    delta = np.zeros(np.shape(delta_fs[0]))\n",
    "                    for df in delta_fs:\n",
    "                        delta = delta + df\n",
    "                    delta = delta.flatten()[:, np.newaxis] \n",
    "                    \n",
    "            #Fully connected layer calculations.\n",
    "            else:   \n",
    "                #Using the current layer delta to calculate grad_b and grad_w for a fully connected layer.\n",
    "                grad_b[-l] = delta\n",
    "                grad_w[-l] = delta@activations[-l-1].T   #Again this creates a matrix since we are multiplying our vectors lengthways.\n",
    "                #We then calculate the value of delta to be passed down from this layer to the next. Note that for our last layer we can skip this step as there is no further layer to pass on a delta to.\n",
    "                if l != self.num_layers-1: delta = (self.weights[-l].T@delta)*self.activation_function_prime(z)  \n",
    "            #If dropout was used on a layer in our forward pass then we apply dropout to that layer's delta which makes the entries of delta corresponding to any dropped nodes equal to zero.\n",
    "            if self.dropout_frequency > 0:\n",
    "                if l != self.num_layers-1 and l != 1: \n",
    "                    if masks[-l+1] is not None:\n",
    "                        delta = delta * masks[-l+1] #Applying dropout to hidden layer deltas on the backward pass.\n",
    "        return (grad_b, grad_w) #Finally we return a tuple of our cost gradient vectors wrt our biases and matrices wrt our weights for our whole network.\n",
    "    \n",
    "    \n",
    "    #This function performs the feed-forward and back-propagation algorithm on an entire mini-batch at once, working through one layer at a time. This tends to run faster than the regular backprop routine.\n",
    "    #Note that this is required for batch normalisation (although I didn't make this compatible with convolutional layers because that would have been too much of a headache).\n",
    "    def batch_backprop(self, mini_batch):        \n",
    "        #Initialising arrays to store our partial derivatives of C wrt weights and biases.\n",
    "        if self.batch_normalisation == False:\n",
    "            grad_b = [np.array(np.zeros(np.shape(b)),dtype=np.float32) for b in self.biases] \n",
    "            grad_w = [np.array(np.zeros(np.shape(w)),dtype=np.float32) for w in self.weights]\n",
    "        mb_activation, mb_y = [], [] #Creating array to hold the activations \n",
    "        \n",
    "        #Concatenating the input activations from our mini-batch into one large activation.\n",
    "        m = len(mini_batch)\n",
    "        for x, y in mini_batch:\n",
    "            if len(mb_activation) == 0: mb_activation = x\n",
    "            else: mb_activation = np.concatenate((mb_activation, x))\n",
    "            if len(mb_y) == 0: mb_y = y\n",
    "            else: mb_y = np.concatenate((mb_y, y))\n",
    "        final_mb_zs = []\n",
    "        \n",
    "        #Batch normalisation calculations.\n",
    "        if self.batch_normalisation == True:\n",
    "            #Initialising arrays to store our partial derivatives of C wrt weights and biases.\n",
    "            grad_w = [np.array(np.zeros(np.shape(w)),dtype=np.float32) for w in self.weights] #Initialises the gradient vector for our weights.\n",
    "            grad_b = [np.array(np.zeros(np.shape(b)),dtype=np.float32) for b in self.biases] #Initialises the gradient vector for our biases.\n",
    "            grad_gamma = [] \n",
    "            grad_beta = []\n",
    "            layer_means = []\n",
    "            layer_variances = []\n",
    "            normalised_mb_zs = [] #Array to store the normalised and fully batch normalised mini batch weighted inputs after performing batch normalisation.\n",
    "            #Batch normalising the input activations and saving the means and variances of these activations.\n",
    "            mb_activation, normalised_mb_z, means, variances = batch_normalise(mb_activation, self.epsilon, self.gammas[0], self.betas[0], m)\n",
    "            normalised_mb_zs.append(normalised_mb_z)\n",
    "            layer_means.append(means)  #We save the mean and variance from this layer's batch normalisation for use in backpropagation later.\n",
    "            layer_variances.append(variances)\n",
    "        final_mb_zs.append(mb_activation)\n",
    "        mb_activations = [mb_activation] #This creates a matrix to store each of our layer activations.\n",
    "        mb_zs = [] #Array to store all of our weighted input vectors z (or z^l).\n",
    "        max_pooling_indices = [] #This creates an array to keep track of any maximal indices used in any max pooling steps we perform (we need these later for backpropagtion).\n",
    "        \n",
    "        #Feedforward part of our batch_backprop routine.\n",
    "        layer = 0 #Current layer counter.\n",
    "        for b, w in zip(self.biases, self.weights):   #The arrays of both the bias vectors and weight matrices have length num_layers-1 since there are no first layer biases and no final layer weights.\n",
    "            max_z_indices = []\n",
    "            layer_size = np.shape(w)[1]\n",
    "            mb_z = []\n",
    "            \n",
    "            #Batch normalisation calculations (we don't batch normalise the final layer).\n",
    "            if self.batch_normalisation == True and layer < self.num_layers-3:\n",
    "                for i in range(m):\n",
    "                    z = w@mb_activation[i*layer_size:(i+1)*layer_size]   #If we are using batch normalisation then we don't need to use biases (they cancel out in the batch normalisation step).\n",
    "                    if len(mb_z) == 0: mb_z = z\n",
    "                    else: mb_z = np.concatenate((mb_z, z))\n",
    "                mb_zs.append(mb_z)\n",
    "                gamma = self.gammas[layer+1]\n",
    "                beta = self.betas[layer+1]\n",
    "                #Batch normalising this layer of weighted inputs over our mini-batch.\n",
    "                mb_z, normalised_mb_z, means, variances = batch_normalise(mb_z, self.epsilon, gamma, beta, m)\n",
    "                #We save the mean, variance and normalised mini-batch weighted inputs from this layer's batch normalisation for use in backpropagation later.\n",
    "                layer_means.append(means) \n",
    "                layer_variances.append(variances)\n",
    "                normalised_mb_zs.append(normalised_mb_z)\n",
    "                final_mb_zs.append(mb_z)\n",
    "                \n",
    "            #Calculations without batch normalisation.\n",
    "            else: \n",
    "                for i in range(m):\n",
    "                    #Calculating our weighted input for each example in our mini-batch\n",
    "                    z = w@mb_activation[i*layer_size:(i+1)*layer_size]+b \n",
    "                    if len(mb_z) == 0: mb_z = z\n",
    "                    else: mb_z = np.concatenate((mb_z, z))\n",
    "                mb_zs.append(mb_z) \n",
    "                final_mb_zs.append(mb_z)\n",
    "            #If we are training a network to play a game then we don't apply our activation function to our final layer weighted inputs so that our outputs take the form of values.\n",
    "            if layer == self.num_layers-2 and self.game != None:  mb_activation = mb_z   \n",
    "            #Here we apply our activation function which introduces non-linearity into our network.\n",
    "            else: mb_activation = self.activation_function(mb_z)  \n",
    "            mb_activations.append(mb_activation)\n",
    "            max_pooling_indices.append(max_z_indices)\n",
    "            layer = layer + 1\n",
    "            \n",
    "        #Backpropagation part of our batch_backprop routine.\n",
    "        #First we calculate the error in the final layer using our mini-batch output activations and the derivative of our cost function.\n",
    "        if self.game != None: mb_delta = self.cost_derivative(mb_activations[-1], mb_y) #If we are using a network to play games then we want to use a linear final layer since predicting the state values is a regression problem.\n",
    "        else: mb_delta = self.cost_derivative(mb_activations[-1], mb_y)*self.activation_function_prime(mb_zs[-1]) #Calculates our final layer error vector over our whole mini-batch.\n",
    "        #Here we backpropagate one layer at a time to calculate the error delta for each layer and then we calculate our cost gradient vectors grad_b and grad_w for each layer.\n",
    "        #First we use our last layer delta to calculate the next delta and pass it on: since the last layer is forced to not be convolutional this is just our standard delta calculation.\n",
    "        for l in range(1, self.num_layers): \n",
    "            if l != self.num_layers-1: mb_z = mb_zs[-l-1] #Unless we are on the first layer we will need our previous layer weighted input at each step.\n",
    "            #Using the current layer delta to calculate grad_b and grad_w for a fully connected layer.\n",
    "            w_shape = np.shape(self.weights[-l])\n",
    "            layer_size = w_shape[1]\n",
    "            next_layer_size = w_shape[0]\n",
    "            #Calculating the small changes required for this layer's weights (these are needed regardless of whether we use batch normalisation or not).\n",
    "            mb_grad_w = np.zeros((m, w_shape[0], w_shape[1]))\n",
    "            mb_grad_y = mb_delta\n",
    "            \n",
    "            #Batch normalisation calculations.\n",
    "            if self.batch_normalisation == True:\n",
    "                #If we are using batch normalisation then there are many intermediate calculations we need to do before calculating the previous layer's value of mb_delta.    \n",
    "                if l > 2: #For all layers but the final two we recieve an mb_delta that represents dC/dy, hence to recover our actual mb_delta=dC/dx we need to use the batch normalisation backprop equations.\n",
    "                    #First we define some quantities ahead of time that we will be repeatedly using to save time.\n",
    "                    inv_m = 1/m\n",
    "                    mb_inv_sqrt_variances = np.tile((layer_variances[-l+2] + self.epsilon)**-0.5, (m,1))\n",
    "                    #We then calculate mb_delta=dC/dx to be passed down to the next layer.\n",
    "                    #This involves the calculation of many intermediate quantities according to the chain rule.\n",
    "                    mb_z = mb_zs[-l]\n",
    "                    final_mb_z = final_mb_zs[-l+2]\n",
    "                    mb_means = np.tile(layer_means[-l+2], (m,1))\n",
    "                    mb_gammas = np.tile(self.gammas[-l+2], (m,1))\n",
    "                    grad_normalised_mb_z = mb_gammas*mb_grad_y\n",
    "                    grad_variances = -0.5*np.reshape(np.sum(np.reshape(grad_normalised_mb_z*(mb_z-mb_means)*mb_inv_sqrt_variances**(3), (m, next_layer_size)), 0), (next_layer_size, 1))\n",
    "                    grad_means = np.sum(np.reshape(-grad_normalised_mb_z*mb_inv_sqrt_variances, (m, next_layer_size)), 0, keepdims=True).T + grad_variances*np.sum(-2*(mb_z-mb_means), 0, keepdims=True)*inv_m\n",
    "                    mb_grad_means = np.tile(grad_means, (m,1))\n",
    "                    mb_grad_variances = np.tile(grad_variances, (m,1))\n",
    "                    grad_mb_z = grad_normalised_mb_z*mb_inv_sqrt_variances + 2*mb_grad_variances*(mb_z-mb_means)*inv_m + mb_grad_means*inv_m\n",
    "                    mb_delta = grad_mb_z\n",
    "            \n",
    "            #We can then calculate our partial derivatives of C wrt our weights and biases for each example in the mini-batch.\n",
    "            for i in range(m):\n",
    "                mb_grad_w[i] = mb_delta[i*next_layer_size:(i+1)*next_layer_size]@np.reshape(mb_activations[-l-1].T[0][i*layer_size:(i+1)*layer_size], (1, layer_size))\n",
    "            reshaped_mb_grad_w = np.reshape(mb_grad_w, (m, next_layer_size, layer_size))\n",
    "            grad_w[-l] = np.sum(reshaped_mb_grad_w, 0)\n",
    "            reshaped_mb_delta = np.reshape(mb_delta, (m, next_layer_size))\n",
    "            grad_b[-l] = np.sum(reshaped_mb_delta, 0)\n",
    "            grad_b[-l] = grad_b[-l][:, np.newaxis]\n",
    "            mb_z = final_mb_zs[-l-1]\n",
    "            new_mb_delta = []\n",
    "            \n",
    "            #Calculating the mini-batch error vector mb_delta to be passed down to the next layer.\n",
    "            for i in range(m): \n",
    "                if l == self.num_layers-1:\n",
    "                    delta = self.weights[-l].T@mb_delta[i*next_layer_size:(i+1)*next_layer_size]    #Calculating our weighted input.\n",
    "                else:\n",
    "                    delta = (self.weights[-l].T@mb_delta[i*next_layer_size:(i+1)*next_layer_size])*self.activation_function_prime(mb_z[i*layer_size:(i+1)*layer_size])    #Calculating our weighted input.\n",
    "                if len(new_mb_delta) == 0: new_mb_delta = delta\n",
    "                else: new_mb_delta = np.concatenate((new_mb_delta, delta))\n",
    "            mb_delta = new_mb_delta\n",
    "            \n",
    "            #Calculating the derivatives of C wrt our batch normalisation parameters beta and gamma.\n",
    "            if self.batch_normalisation == True and l > 2:\n",
    "                reshaped_mb_grad_y = np.reshape(mb_grad_y, (m, next_layer_size))\n",
    "                grad_beta.insert(0, np.sum(reshaped_mb_grad_y.T, 1, keepdims=True))\n",
    "                reshaped_normalised_mb_z = np.reshape(normalised_mb_zs[-l+2], (m, next_layer_size))\n",
    "                grad_gamma.insert(0, np.sum((reshaped_mb_grad_y*reshaped_normalised_mb_z).T, 1, keepdims=True))\n",
    "        mb_grad_y = mb_delta\n",
    "        \n",
    "        #Calculating the derivatives of C wrt our batch normalisation parameters beta and gamma for our input layer. \n",
    "        if self.batch_normalisation == True:\n",
    "            reshaped_mb_grad_y = np.reshape(mb_grad_y, (m, layer_size))\n",
    "            grad_beta.insert(0, np.sum(reshaped_mb_grad_y.T, 1, keepdims=True))\n",
    "            reshaped_normalised_mb_z = np.reshape(normalised_mb_zs[0], (m, layer_size))\n",
    "            grad_gamma.insert(0, np.sum((reshaped_mb_grad_y*reshaped_normalised_mb_z).T, 1, keepdims=True))\n",
    "            \n",
    "        #Finally we return a tuple of our cost gradient vectors wrt our biases and matrices wrt our weights for our whole network.\n",
    "        if self.batch_normalisation == False: return (grad_b, grad_w)\n",
    "        #If we are using batch normalisation then we instead return the cost gradient vectors wrt our weights, biases, gammas and betas (although the biases go unused).\n",
    "        if self.batch_normalisation == True: return (grad_gamma, grad_beta, grad_b, grad_w, layer_means, layer_variances) \n",
    "\n",
    "    #Function for evaluating a network's performance on unseen test data.\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    #Derivative of the L^2 cost.\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    #Code for displaying the learned weights of each filter in a network's convolutional layers.\n",
    "    def display_weights(self, plot = True):\n",
    "        for l in range(self.num_layers):\n",
    "            layer = self.layers[l]\n",
    "            if type(layer) == list:\n",
    "                filter_size = layer[2]\n",
    "                i = 1\n",
    "                for w in self.weights[l]:\n",
    "                    if plot == True:\n",
    "                        filter_plot(w, i)\n",
    "                    else:\n",
    "                        w = np.reshape(w, (filter_size, filter_size))\n",
    "                        print(\"Filter \",i,\" weights:\")\n",
    "                        print(w)\n",
    "                        print(\"\")\n",
    "                    i = i + 1\n",
    "        return\n",
    "\n",
    "    #Code for copying networks if needed.\n",
    "    def copy_network(self):\n",
    "        network_copy = Network(self.layers, self.activation_function, self.padding, self.dropout_frequency, self.batch_normalisation, self.game, self.exploration_type, self.data_augmentation)\n",
    "        network_copy.weights = copy.deepcopy(self.weights)\n",
    "        network_copy.biases = copy.deepcopy(self.biases)\n",
    "        network_copy.experiences = copy.deepcopy(self.experiences)\n",
    "        return network_copy\n",
    "    \n",
    "    #Code that allows us to change the exploration strategy of a network after it's been created.\n",
    "    def set_exploration_strategy(self, new_type, new_parameter): \n",
    "        self.exploration_parameter = new_parameter   #Changing epsilon or the exploration temperature.\n",
    "        self.exploration_type = new_type  #Exploration types available are: 'epsilon-greedy', 'softmax' and 'adaptive-greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "178023aa-61cd-4dd4-b53b-e16e24bc312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing matplotlib for making plots.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Creating a function for creating plots of networks' accuracies during training.\n",
    "def plot_accuracies(accuracies, color, yrange, single_epoch = False):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    if single_epoch == True:\n",
    "        barwidth = 1500\n",
    "        training_examples_seen = np.arange(0, 50001, 2000)\n",
    "        plt.bar(training_examples_seen, accuracies, barwidth, color=color, edgecolor='black')\n",
    "        plt.xlabel(\"Training examples seen\", fontsize = 14)\n",
    "        plt.xlim([0-barwidth, 50000+barwidth])\n",
    "    else: \n",
    "        epochs = np.arange(0,len(accuracies))\n",
    "        plt.plot(epochs, accuracies, color=color)\n",
    "        plt.xlabel(\"Training epochs (i.e. runs over the training set)\", fontsize = 14)\n",
    "        plt.xlim([0, len(accuracies)-1])\n",
    "        plt.grid()\n",
    "    #Adding a black horizontal line to mark the 10% accuracy obtainable via random chance.\n",
    "    plt.plot([-5000, 55000], [10, 10], 'k', linewidth=1.5)\n",
    "    plt.ylim(yrange)\n",
    "    #Labelling axes and framing plot window.\n",
    "    plt.ylabel(\"% accuracy on unseen test data\", fontsize = 14)\n",
    "    plt.show()\n",
    "    \n",
    "#Creating a function for creating a combined plot of three networks' accuracies.\n",
    "def plot_multi_accuracies(accuracy_array, colors, legend, yrange):\n",
    "    epochs = np.arange(0,len(accuracies1))\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    plt.grid()\n",
    "    for (accuracy, color) in zip(accuracy_array, colors):\n",
    "        plt.plot(epochs, accuracy, color=color)\n",
    "    plt.legend(legend, loc=\"best\", fontsize=12)\n",
    "    #Adding a black horizontal line to mark the 10% accuracy obtainable via random chance.\n",
    "    plt.plot([0, len(accuracy_array[0])-1], [10, 10], 'k', linewidth=1.5)\n",
    "    #Labelling axes and framing plot window.\n",
    "    plt.xlim([0, len(accuracy_array[0])-1])\n",
    "    plt.ylim(yrange)\n",
    "    plt.xlabel(\"Training epochs\", fontsize = 14)\n",
    "    plt.ylabel(\"% accuracy on unseen test data\", fontsize = 14)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#Function for converting MNIST image data into the actual image.\n",
    "def MNIST_plot(data, number, showLabels = False):\n",
    "    fig, ax = plt.subplots()\n",
    "    image = np.reshape(data,(28, 28))\n",
    "    ax.imshow(image, cmap=plt.cm.binary)\n",
    "    if showLabels == True:\n",
    "        #Listing our labels (only used if specified in our argument)\n",
    "        labels = ['Zero', 'One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine']\n",
    "        for j in range(10):\n",
    "            if data[1][j][0] == 1: #Here we check each entry of the vectorised form of our output (i.e. [0, ..., 1, ... 0]) to determine which label we need\n",
    "                label = labels[j]\n",
    "        plt.title(\"Image {:} - {:}\".format(number+1,label), fontsize = 15)\n",
    "    else:\n",
    "        plt.title(\"Image {:}\".format(number+1), fontsize = 15)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.show()\n",
    "    \n",
    "#Function for visualising the weights learned by a convolutional filter.\n",
    "def filter_plot(filter, number):\n",
    "    fig, ax = plt.subplots()\n",
    "    filter_size = int(np.sqrt(np.size(filter)))\n",
    "    #Reshaping the filter weights into the filter shape.\n",
    "    image = np.reshape(filter,(filter_size, filter_size))\n",
    "    ax.imshow(image, cmap=plt.cm.binary)\n",
    "    plt.title(\"Filter {:}\".format(number), fontsize = 15)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3bed3c-e933-4335-8913-e85efada9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the training and test data.\n",
    "training_data, test_data = load_data_wrapper()\n",
    "#Here I also define small and medium versions of the training and test data that can be used for faster but less accurate training.\n",
    "training_data_small = random.sample(training_data, 500)\n",
    "training_data_medium = random.sample(training_data, 5000)\n",
    "test_data_small = random.sample(test_data, 100)\n",
    "test_data_medium = random.sample(test_data, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
