{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea217c6-e5bb-4e41-a103-a7ae56c7f364",
   "metadata": {},
   "source": [
    "### Reinforcement Learning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b72963-2f7b-4633-8114-84f3fb0aec23",
   "metadata": {},
   "source": [
    "This code contains the Qlearn routine which is used for training neural networks to play games using reinforcement learning.\n",
    "The two games I've implemented with this are Connect 4 and Nought and Crosses which are found in the script GameCode.ipynb.\n",
    "This script also contains routines for pitting two networks against eachother in evaluation games (i.e. after learning) as well as a routine for letting a human play against a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7697f201-5275-491f-89bc-2ede9f6c534d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for training two networks against eachother using deep Q-learning.\n",
    "#Here p1 or p2 should be neural networks (i.e. an object from the Network class in the Neural Networks.ipynb).\n",
    "#These could either be newly created networks or pre-existing networks that you want to re-train.\n",
    "#We can also choose to enter the string \"random\" for either player to train the network against a random opponent.\n",
    "\n",
    "#Our other parameters are:\n",
    "#total_games: total number of games to be played during training; here the networks are updated once per move;\n",
    "#eta: learning rate for networks;\n",
    "#gamma: discounting factor (1 = no discounting, 0 = full discounting);\n",
    "#evaluate: this is a flag that specifies whether to evaluate our networks against a random opponent at every 10% training interval;\n",
    "#lmbda: L2 regularisation hyperparameter for weight regularisation of neural networks;\n",
    "\n",
    "def Qlearn(total_games, eta, gamma, p1, p2, evaluate = False, save_stats = False, lmbda = 0):\n",
    "    \n",
    "    #First we check that the two networks want to play the same game.\n",
    "    if p1 != \"random\" and p2 != \"random\":  \n",
    "        if p1.game != p2.game: \n",
    "            print(\"Error: Opponents are trying to play different games\")\n",
    "            return\n",
    "        else: game = p1.game\n",
    "    else: \n",
    "        if p1 != \"random\": game = p1.game\n",
    "        else: game = p2.game\n",
    "    (p1_score, p2_score) = (0, 0) #Setting the initial scores to be 0-0.\n",
    "    max_games = total_games  #Saving the number of games to be played overall (as total_games will be decremented later).\n",
    "    standard_reward = 1   #Setting the standard reward/penalty given out during training after networks make winning/losing moves or invalid moves.\n",
    "    \n",
    "    #Preparing vectors for saving stats.\n",
    "    if save_stats == True:\n",
    "        running_scores = np.zeros((total_games,2))\n",
    "        vs_games = []\n",
    "        #We also each network's winrate against a random opponent before any training as a 'control test'.\n",
    "        scores_vs_random = np.zeros((11, 2))\n",
    "        (p1_wins, p1_losses, p1_draws) = NetworkVsNetwork(1000, p1, \"random\", False, False, game)\n",
    "        (p2_losses, p2_wins, p2_draws) = NetworkVsNetwork(1000, \"random\", p2, False, False, game)\n",
    "        scores_vs_random[0] = (p1_wins-p1_losses, p2_wins-p2_losses)\n",
    "        i = 1\n",
    "    \n",
    "    #Here we loop over the total number of games to be played in our training session by playing a game to completion (with training) and decrementing total_games until it reaches 0 (at which point we stop training).\n",
    "    while total_games > 0: \n",
    "        board, game_over, current_turn = reset_game(game)  #Resetting the turn number and board for each new game.\n",
    "        (current_player, waiting_player) = (p1, p2)  #Setting p1 to always be the starting player.\n",
    "        #Here we loop over an individual game, during which we get our networks to choose moves and we update the networks according to the observed rewards and values of their chosen moves.\n",
    "        while game_over == False: #As long as the game hasn't been ended by the previous player then the new current player chooses their move.\n",
    "            initial_state = board.flatten()  #Storing and reshaping the initial board state into a vectorised form which is presentable to the current network player.\n",
    "            chosen_action, action_values = select_move(current_player, initial_state, eta, lmbda, game, standard_reward)  #Here we get our network to choose a move based on the current board.\n",
    "            initial_scores = (p1_score, p2_score)  #Recording the scores before the next move is made.\n",
    "            board, current_turn, p1_score, p2_score, game_over = input_move(chosen_action, False, game, board, current_turn, p1_score, p2_score)    #Inputs the chosen action and updates the board.\n",
    "            \n",
    "            #If the current player's move has ended the game then we need to update both player's most recent moves with an instantaneous reward/penalty respectively.\n",
    "            if game_over == True: #Also note that since a player can't lose on their own turn, the game must have ended by the current player winning or by a draw.\n",
    "                if (p1_score, p2_score) == initial_scores: reward = 0  #In the case of a draw (i.e. unchanged scores) we give both players a neutral 'reward' of 0.\n",
    "                else: reward = standard_reward #Otherwise the current player must have won so we give them the standard reward (as specified above) and we penalise the waiting player by an equal amount.                    \n",
    "                #Performing a single step of gradient descent on the current player (i.e. the winning player)\n",
    "                if current_player != \"random\": \n",
    "                    #Adding an extra dimension to wp_inital_state so it can be fed-forward through our network.\n",
    "                    initial_state = initial_state[:, np.newaxis]\n",
    "                    #Here we use the experience to generate our target values for each action.\n",
    "                    target_values = get_target_values(current_player, initial_state, chosen_action, reward, None, gamma, game, standard_reward)\n",
    "                    #Here we can treat this single experience as a mini-batch of size 1 and perform gradient descent as usual.\n",
    "                    #In our gradient descent step, initial_state takes the place of our 'input' and the target_values form our 'label'.\n",
    "                    mb_means, mb_variances = current_player.mini_batch_gradient_descent([(initial_state, target_values)], eta, lmbda, 1) \n",
    "                #Performing a single step of gradient descent on the waiting player (i.e. the losing player)\n",
    "                if waiting_player != \"random\": \n",
    "                    #Adding an extra dimension to wp_inital_state so it can be fed-forward through our network.\n",
    "                    wp_initial_state = wp_initial_state[:, np.newaxis]\n",
    "                    #Here we use the experience to generate our target values for each action.               \n",
    "                    target_values = get_target_values(waiting_player, wp_initial_state, wp_chosen_action, -reward, None, gamma, game, standard_reward)\n",
    "                    #Here we can treat this single experience as a mini-batch of size 1 and perform gradient descent as usual.\n",
    "                    #In our gradient descent step, initial_state takes the place of our 'input' and the target_values form our 'label'.\n",
    "                    mb_means, mb_variances = waiting_player.mini_batch_gradient_descent([(wp_initial_state, target_values)], eta, lmbda, 1) \n",
    "                    \n",
    "            #If the game hasn't ended then the state created by the current player's move can now be used to update the values of the waiting player's chosen move (or specifically to update the waiting player's network).\n",
    "            else:\n",
    "                #We need this state since this is the next actionable state for the waiting player which is used to get the percieved value of this new state as required in the update step for the waiting player's network. \n",
    "                if current_turn != 2 and waiting_player != \"random\":    #We don't perform our network update if our network is the random player or if it's the second turn in the game since then the waiting player hasn't made any moves whose values need to be updated.\n",
    "                    next_state = board.flatten()\n",
    "                    #Adding an extra dimension to wp_inital_state so it can be fed-forward through our network.\n",
    "                    wp_initial_state = wp_initial_state[:, np.newaxis]\n",
    "                    #Here we use the experience to generate our target values for each action.\n",
    "                    target_values = get_target_values(waiting_player, wp_initial_state, wp_chosen_action, 0, next_state, gamma, game, standard_reward)\n",
    "                    #Here we can treat this single experience as a mini-batch of size 1 and perform gradient descent as usual.\n",
    "                    #In our gradient descent step, initial_state takes the place of our 'input' and the target_values form our 'label'.\n",
    "                    mb_means, mb_variances = waiting_player.mini_batch_gradient_descent([(wp_initial_state, target_values)], eta, lmbda, 1) \n",
    "\n",
    "            #After each turn we swap the active player with the waiting player.\n",
    "            (current_player, waiting_player) = (waiting_player, current_player) \n",
    "            #Saving the most recent player's presented board state and their associated chosen action for use in our next update step.\n",
    "            wp_initial_state = initial_state\n",
    "            wp_chosen_action = chosen_action\n",
    "\n",
    "        if total_games%(max_games*0.1) == 0 or total_games == 1:\n",
    "            #Here we can choose to evaluate our networks against random opponents at each 10% training interval.\n",
    "            if total_games != max_games and evaluate == True:\n",
    "                (p1_wins, p1_losses, p1_draws) = NetworkVsNetwork(1000, p1, \"random\", False, False, game)\n",
    "                (p2_losses, p2_wins, p2_draws) = NetworkVsNetwork(1000, \"random\", p2, False, False, game)\n",
    "                if save_stats == True:\n",
    "                    scores_vs_random[i] = (p1_wins-p1_losses, p2_wins-p2_losses)\n",
    "                    i += 1\n",
    "                    #Here we also save a replay of a game between the networks at each 5% training interval.\n",
    "                    game_replay = NetworkVsNetwork(1, p1, p2, False, True, game)\n",
    "                    vs_games.append(game_replay)\n",
    "                \n",
    "            #Then we print out how many games are left at intervals of 5% of the total games to be played\n",
    "            if total_games != 1: print(\"Games left: \",total_games)    \n",
    "        \n",
    "        #Saving the running scores.\n",
    "        if save_stats == True: running_scores[max_games-total_games] = (p1_score, p2_score)\n",
    "        total_games = total_games - 1  #Decrementing total_games after a game has been played to completion\n",
    "\n",
    "    if save_stats == True: return (vs_games, running_scores, scores_vs_random)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04951907-278d-49a1-bd29-b5916a6b1207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Code for selecting a move during the training of a neural network.\n",
    "#The board state will be fed-forward through the network to produce the networks estimates of each action's action-values.\n",
    "#A move will be chosen based on these values and on the network's exploration/exploitation strategy.\n",
    "def select_move(player, state, eta, lmbda, game, standard_reward):\n",
    "    state = state[:, np.newaxis]\n",
    "    if player != \"random\": action_values = player.feedforward(state)   #Here we feed the current board state into our network and at returns the values it estimates each action to have in this state.\n",
    "    if player == \"random\": action_values = None   #If we are playing against a random network then we don't need the action values as no update step is performed on our random opponent.\n",
    "    valid_moves = get_valid_moves(state, game)    #Creates a list of the indices for valid moves to be made, i.e. squares where our board is empty for noughts and crosses or where a column isn't full for connect 4.\n",
    "    \n",
    "    #If we entered \"random\" as our network then our move is always chosen randomly.\n",
    "    if player == \"random\":       \n",
    "        chosen_action = random.choice(valid_moves) \n",
    "        \n",
    "    #For epsilon-greedy exploration we simply explore randomly with probability epsilon each move (here epsilon is denoted by exploration_parameter).\n",
    "    elif player.exploration_type == \"epsilon-greedy\":\n",
    "        if np.random.uniform() < player.exploration_parameter:\n",
    "            chosen_action = random.choice(valid_moves) \n",
    "        else:\n",
    "            #As mentioned above, we usually \"exploit\" and choose the action with the highest value in our state.\n",
    "            chosen_action = np.argmax(action_values) \n",
    "            #In the case that the network chooses an invalid move treat this as a loss and punish the network.\n",
    "            #We then allow the game to continue by forcing a valid move to be chosen instead.\n",
    "            while chosen_action not in valid_moves:   \n",
    "                #Here we treat this invalid move as a loss and strongly punish the network with a reward of -10 and a single step of gradient descent.\n",
    "                target_values = get_target_values(player, state.flatten()[:, np.newaxis], chosen_action, -5*standard_reward, None, 0, game, standard_reward)\n",
    "                mb_means, mb_variances = player.mini_batch_gradient_descent([(state.flatten()[:, np.newaxis], target_values)], eta, lmbda, 1) \n",
    "                #Creating a temporary copy of action values which we will use to ensure that a valid action is chosen next.\n",
    "                action_values_copy = action_values.copy()   \n",
    "                #Having punished the network we then ensure that the network picks a valid move so that the game can continue.\n",
    "                for index in valid_moves:   \n",
    "                    action_values_copy[index] = action_values_copy[index] + 10   #To ensure that the network now picks a valid move we temporarily increase only the valid move values by 10.\n",
    "                chosen_action = np.argmax(action_values_copy)\n",
    "                \n",
    "    return chosen_action, action_values\n",
    "    \n",
    "#Here we determine the vector of target values used in our backpropagation step for reinforcement learning. \n",
    "#The action-values for the unchosen action are typically unchanged unless they exceed the expected range (-1, 1).\n",
    "def get_target_values(network, initial_state, chosen_action, reward, next_state, gamma, game, standard_reward):  \n",
    "    #The only update we potentially do to the unchosen move values is to set their target value to be in the range (-1, 1) if their current value slightly exceeds this limit (usually they will be within this range and this step won't do anything).\n",
    "    initial_values = network.feedforward(initial_state) \n",
    "    #Ensuring that our target values don't go above 1 or below -1.\n",
    "    initial_values = np.where(abs(initial_values)>standard_reward, np.sign(initial_values)*standard_reward, initial_values)  \n",
    "    #If the value of next_state is None then we set the value of the next state to 0 since it's a terminal state.\n",
    "    if next_state is None: next_state_max_value = 0   \n",
    "    #Otherwise we calculate the maximum action value attainable from actions in our next state.\n",
    "    else: \n",
    "        next_state_values = network.feedforward(next_state)\n",
    "        next_state_valid_moves = get_valid_moves(next_state, game)\n",
    "        valid_move_values = []\n",
    "        for index in next_state_valid_moves: valid_move_values.append(next_state_values[index])\n",
    "        next_state_max_value = np.max(valid_move_values)\n",
    "    target_values = initial_values\n",
    "    #Here we set the target value of our chosen action using the RHS of the Bellman optimality equation (for action-values).\n",
    "    target_values[chosen_action] = reward + gamma*next_state_max_value     \n",
    "    return target_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "512a466c-6178-4407-8851-de77dbecb59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code that allows two networks to play against each other at either \"noughts and crosses\" or \"connect 4\".\n",
    "#total_games is the number of games to be played and also a random opponent can be specified using the string \"random\".\n",
    "#If show_game is set to True then each turn of the game will be printed out:\n",
    "#Here player 1's moves are represented by +1 on the board and player 2's moves are represented by -1.\n",
    "\n",
    "def NetworkVsNetwork(total_games, p1, p2, show_game, save_game, game):\n",
    "    \n",
    "    #Resetting the scores; these will be kept track of over all the games .\n",
    "    (p1_score, p2_score) = (0, 0)\n",
    "    max_games = total_games\n",
    "    \n",
    "    #Loop over all games to be played.\n",
    "    while total_games > 0:\n",
    "        #Resetting the turn number and board for each new game and setting p1 to always be the starting player.\n",
    "        board, game_over, current_turn = reset_game(game) \n",
    "        (current_player, waiting_player) = (p1, p2)\n",
    "        if save_game == True and max_games == 1: game_replay = [np.copy(board)]\n",
    "        #Game loop.\n",
    "        while game_over == False: #As long as the game hasn't been ended by p2 then p1 chooses their move\n",
    "            state = np.reshape(board, (np.size(board), 1))\n",
    "            valid_moves = get_valid_moves(state, game) \n",
    "            if current_player != \"random\":\n",
    "                #Here we are no longer exploring so we always choose the greedy move.\n",
    "                initial_values = current_player.feedforward(state)\n",
    "                chosen_action = np.argmax(initial_values)\n",
    "                while chosen_action not in valid_moves:\n",
    "                    for index in valid_moves:\n",
    "                        initial_values[index] = initial_values[index] + 10\n",
    "                    chosen_action = np.argmax(initial_values)\n",
    "            else:\n",
    "                chosen_action = random.choice(valid_moves)\n",
    "            #Inputting the network's move to the board (or a random move if using a random opponent).\n",
    "            board, current_turn, p1_score, p2_score, game_over = input_move(chosen_action, show_game, game, board, current_turn, p1_score, p2_score)\n",
    "            #Saving a replay of the game if specified.\n",
    "            if save_game == True and max_games == 1: game_replay.append(np.copy(board))\n",
    "            #After each turn we swap the active player with the waiting player and continue the game.\n",
    "            (current_player, waiting_player) = (waiting_player, current_player)      \n",
    "        total_games = total_games-1\n",
    "    \n",
    "    draws = max_games-p1_score-p2_score\n",
    "    if show_game == True:\n",
    "        print(\"\")\n",
    "        print(\"Final scores (p1, p2): (\",p1_score,\"-\",p2_score,\")\")\n",
    "        print(\"Draws = \",draws)\n",
    "        print(\"\")\n",
    "    if save_game == True and max_games == 1: return game_replay\n",
    "    return (p1_score, p2_score, draws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "83700489-8b7c-4ec9-9424-60f25834b64f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Code that allows a human to play against a neural network at either \"noughts and crosses\" or \"connect 4\".\n",
    "#Here player 1's moves are represented by +1 on the board and player 2's moves are represented by -1.\n",
    "\n",
    "def HumanVsNetwork(p1, p2, game):\n",
    "    \n",
    "    #Resetting the turn number and board for each new game and setting p1 to always be the starting player.\n",
    "    board, game_over, current_turn = reset_game(game) \n",
    "    p1_score, p2_score = 0, 0\n",
    "    (current_player, waiting_player) = (p1, p2) \n",
    "    #Game loop; as long as the game hasn't been ended by the previous player then the new current player chooses their move.\n",
    "    while game_over == False:\n",
    "        #Reshaping the board so it's ready to be printed for the player to see.\n",
    "        if game == \"noughts and crosses\":\n",
    "            print_board = np.reshape(board, (3,3))\n",
    "        if game == \"connect 4\":\n",
    "            print_board = np.reshape(board, (6,7))\n",
    "        valid_moves = get_valid_moves(board, game)\n",
    "        \n",
    "        #Human's turn.\n",
    "        if current_player == \"human\":\n",
    "            print(\"Your turn: \\n\",print_board)\n",
    "            if game == \"noughts and crosses\":\n",
    "                #Making sure that the chosen move is valid.\n",
    "                human_move = int(input(\"Choose a square: \"))\n",
    "                while human_move not in [1,2,3,4,5,6,7,8,9]:\n",
    "                    human_move = int(input(\"Invalid move, please choose a number from 1 to 9: \"))\n",
    "                while human_move-1 not in valid_moves:\n",
    "                    human_move = int(input(\"Chosen square is full, please choose another square: \"))\n",
    "            if game == \"connect 4\":\n",
    "                #Making sure that the chosen move is valid.\n",
    "                human_move = int(input(\"Choose a column: \"))\n",
    "                while human_move not in [1,2,3,4,5,6,7]:\n",
    "                    human_move = int(input(\"Invalid move, please choose a number from 1 to 7: \"))\n",
    "                while human_move-1 not in valid_moves:\n",
    "                    human_move = int(input(\"Chosen column is full, please choose another column: \"))\n",
    "            #Since the actual indices start from 0 we have to remove 1 from the selected moves.\n",
    "            chosen_move = human_move - 1\n",
    "            \n",
    "        #Network's turn.\n",
    "        else:\n",
    "            print(\"Network's turn \\n\",print_board)\n",
    "            state = np.reshape(board, (np.size(board), 1))\n",
    "            move_values = current_player.feedforward(state)\n",
    "            chosen_move = np.argmax(move_values)\n",
    "            while chosen_move not in valid_moves:\n",
    "                for index in valid_moves:\n",
    "                    move_values[index] = move_values[index] + 10\n",
    "                chosen_move = np.argmax(move_values)\n",
    "            if game == \"noughts and crosses\":\n",
    "                print(\"Network chose square \",chosen_move+1)\n",
    "            if game == \"connect 4\":\n",
    "                print(\"Network chose column \",chosen_move+1)\n",
    "                \n",
    "        #Inputting either the human's or the network's move to the board.\n",
    "        board, current_turn, p1_score, p2_score, game_over = input_move(chosen_move, False, game, board, current_turn, p1_score, p2_score)\n",
    "        #Here we swap the active player with the waiting player and continue the game.\n",
    "        (current_player, waiting_player) = (waiting_player, current_player)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9a7b0-a309-44d7-85cb-967b23d033db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for plotting win rates against a random opponent obtained during training.\n",
    "def plot_stats(stats):\n",
    "    #Unpacking the stats tuple.\n",
    "    (vs_games, running_scores, scores_vs_random) = stats\n",
    "    total_games = len(running_scores)\n",
    "    \n",
    "    #Plotting of the running scores of each network during all training games.\n",
    "    games_played = np.arange(1,total_games+1)\n",
    "    #Plotting player 1 in red and player 2 in blue.\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(games_played, running_scores[:,0], color='red')\n",
    "    plt.plot(games_played, running_scores[:,1], color='blue')\n",
    "    #Labelling axes and framing plot window.\n",
    "    plt.xlim([0, plt.xlim()[1]])\n",
    "    plt.xticks(np.arange(0,total_games+1, 0.1*total_games))\n",
    "    plt.ylim([0,plt.ylim()[1]])\n",
    "    plt.xlabel(\"Training games played\", fontsize=14)\n",
    "    plt.ylabel(\"Scores during training\",fontsize=14)\n",
    "    plt.legend([\"Player 1 Score\", \"Player 2 Score\"], loc=\"upper left\", fontsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "    #Plotting the winrate of each player against a random opponent at 10% training intervals.\n",
    "    games_played = np.arange(0,total_games+1, 0.1*total_games)\n",
    "    bar_width = 0.03*total_games\n",
    "    bar_offset = 0.015*total_games\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(games_played-bar_offset, scores_vs_random[:,0]/1000, bar_width, color='red', edgecolor = 'black') #Plotting player 1 in red.\n",
    "    plt.bar(games_played+bar_offset, scores_vs_random[:,1]/1000, bar_width, color='blue', edgecolor = 'black') #Plotting player 2 in blue.\n",
    "    plt.legend([\"Player 1\", \"Player 2\"], loc=\"upper left\", fontsize=10)\n",
    "    plt.plot([-2*bar_width, total_games+2*bar_width], [0, 0], 'k', linewidth=0.9)\n",
    "    #Labelling axes and framing plot window.\n",
    "    plt.xlim([-2*bar_width, total_games+2*bar_width])\n",
    "    plt.xticks(games_played)\n",
    "    plt.ylim([plt.ylim()[0], 1])\n",
    "    plt.xlabel(\"Training games played\", fontsize = 14)\n",
    "    plt.ylabel(\"Average score against random opponent\", fontsize = 12)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
