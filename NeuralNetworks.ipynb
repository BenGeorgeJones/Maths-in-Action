{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4079aad2-985d-4b54-ab5f-86e1db2faa15",
   "metadata": {},
   "source": [
    "### Neural network training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3103a4-ea61-40d5-a822-29ec56a8ce1a",
   "metadata": {},
   "source": [
    "This code contains the Network class which handles the defining and training of any neural networks we use. In particular this class contains routines such as feed-forward, back-propagation, stochastic gradient descent, network evaluation and more. For this basic version of the neural network code I mostly followed the online tutorial \"Neural Networks and Deep Learning\" by Michael A. Nielsen which was invaluable in understanding the algorithms behind stochastic gradient descent and backpropagation as well as how to implement them into python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b922bd3e-825b-4c78-9700-9ebec56dc5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Importing standard libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#Miscellaneous functions mostly used in the Network class.\n",
    "#Sigmoid activation function.\n",
    "def sigmoid(z): \n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "#Derivative of the sigmoid function; the chain rule shows that it can be expressed as follows in terms of itself.\n",
    "def sigmoid_prime(z):    \n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "    \n",
    "#Function that takes an integer j and returns a 10-entry vector with a 1 in the j^th position and 0 everywhere else.\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4120e63-7048-414c-b28c-14c2edc43d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for importing data sets.\n",
    "import pickle  \n",
    "import gzip\n",
    "\n",
    "#Code for loading the MNIST data into the python runtime. \n",
    "def load_data():\n",
    "    #Opening the file containing the MNIST data.\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    #Loading the 50,000 training examples and the 10,000 validation and test examples.\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    return (training_data, test_data) #Here I don't use validation data and just return the training and test data.\n",
    "\n",
    "#Code for organising the training and test data so that its ready for training use.\n",
    "def load_data_wrapper():\n",
    "    #Loading in the MNIST training data and test data using our load_data() routine above.\n",
    "    tr_d, te_d = load_data()\n",
    "    #Organising the 50,000 training data images from MNIST with their corresponding labels.\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    #For the test data we save the labels in vector form rather than as integers.\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]] \n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    #Organising the 10,000 test data images from MNIST with their corresponding labels.\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68505157-3bcd-4e00-9438-245aa7555b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network class: this is the heart of the code as it contains all functions used in defining and training the neural networks.\n",
    "#This class contains routines such as feed-forward, back-propagation, stochastic gradient descent, network evaluation and more.\n",
    "#The entries of `layers' denote the number of nodes you want in each layer.\n",
    "class Network(object):\n",
    "    def __init__(self, layers): \n",
    "        \n",
    "        #Initialising some useful parameters:\n",
    "        self.num_layers = len(layers) #Number of layers.\n",
    "        self.layers = layers #Array storing the number of nodes per layer.\n",
    "        #Initialising storage for our weights and biases.\n",
    "        self.weights = [None for layer in layers[:-1]]  \n",
    "        self.biases = [None for layer in layers[1:]]\n",
    "\n",
    "        #Looping over each layer in our network to initialise weights and biases in each layer.\n",
    "        for i in range(self.num_layers-1):\n",
    "            if type(layers[i+1]) == list: \n",
    "                next_layer_size = layers[i+1][0]*layers[i+1][1]  #Determining the size needed for inputs to our layer if it's a convolutional layer.                    \n",
    "            else: next_layer_size = layers[i+1]  #Otherwise we have a fully connected layer so its size is simply prescribed by the value of layers[i+1].\n",
    "            self.biases[i] = np.array(np.random.randn(next_layer_size, 1),dtype=np.float32) #Note that we don't define biases for fully connected first layer nodes.\n",
    "            self.weights[i] = np.array(np.random.randn(next_layer_size, layers[i])/np.sqrt(layers[i]),dtype=np.float32) \n",
    "            \n",
    "    #Code for feeding forward an array of input activations 'a' through our network to produce output activations.\n",
    "    def feedforward(self, a):\n",
    "        #Here we loop over each layer in our network.\n",
    "        for b, w in zip(self.biases, self.weights):   #The arrays of both the bias vectors and weight matrices have length num_layers-1 since there are no first layer biases and no final layer weights.\n",
    "            #Calculating our vector of weighted inputs for each layer.\n",
    "            z = w@a+b\n",
    "            #Turning z into a vector of activations using the sigmoid activation function.\n",
    "            a = sigmoid(z)                            \n",
    "        return a\n",
    "\n",
    "    \n",
    "    #Function for training a neural network using stochastic gradient descent for a given number of epochs.\n",
    "    #Here eta is the learning rate and if test_data is provided then the network will evaluate itself against it at the end of each epoch.\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data = None):\n",
    "        \n",
    "        #Recording the size of the test and training data and performing an initial evaluation on the untrained network.\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "            correct = self.evaluate(test_data)\n",
    "            accuracies = [100*correct/n_test]\n",
    "        n = len(training_data)\n",
    "        \n",
    "        #Looping over each epoch (1 epoch = 1 full run through the training data).\n",
    "        for j in range(epochs):\n",
    "            #Creating the mini-batches.\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                #Performing a single gradient descent step for each mini-batch.\n",
    "                self.mini_batch_gradient_descent(mini_batch, eta, n)\n",
    "            #If test data is provided then the network will evaluate its performance against this data after every epoch.\n",
    "            if test_data:\n",
    "                correct = self.evaluate(test_data)\n",
    "                accuracies.append(100*correct/n_test)\n",
    "        if test_data: return accuracies \n",
    "    \n",
    "    #This code tweaks the network's learnable parameters using gradient descent over a single mini-batch.\n",
    "    def mini_batch_gradient_descent(self, mini_batch, eta, n):\n",
    "        #Initialising storage for the gradients of our cost function C wrt the network's weights and biases.\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #Defining a coefficient used in our weight update step\n",
    "        inv_len_mini_batch = 1/len(mini_batch)\n",
    "        #All our gradient terms below (i.e. grad_w, grad_b) use the below coefficient in our update step.\n",
    "        grad_coeff = eta*inv_len_mini_batch \n",
    "        for x, y in mini_batch:\n",
    "            delta_grad_b, delta_grad_w = self.backprop(x, y)\n",
    "            grad_b = [gb+dgb for gb, dgb in zip(grad_b, delta_grad_b)]\n",
    "            grad_w = [gw+dgw for gw, dgw in zip(grad_w, delta_grad_w)]\n",
    "            #Update step for our learnable parameters.\n",
    "            self.weights = [w-grad_coeff*gw for w, gw in zip(self.weights, grad_w)]\n",
    "            self.biases = [b-grad_coeff*gb for b, gb in zip(self.biases, grad_b)]  \n",
    "        return\n",
    "\n",
    "    #Function for back-propagation through our network taking in one mini batch tuple input (x,y) at a time to calculate the gradient\n",
    "    #vectors grad_b and grad_w of our cost function C wrt our weights and biases.\n",
    "    def backprop(self, x, y): \n",
    "        \n",
    "        #Initialising arrays to store our partial derivatives of C wrt weights and biases.\n",
    "        grad_b = [np.array(np.zeros(np.shape(b)),dtype=np.float32) for b in self.biases] \n",
    "        grad_w = [np.array(np.zeros(np.shape(w)),dtype=np.float32) for w in self.weights] \n",
    "        #Here we feedforward to calculate our weighted inputs z^l and our activations a^l := activation_function(z) for each layer l.\n",
    "        #Using these we can then calculate the final layer error vector delta^L and the other layer error vectors delta^l for all l.\n",
    "        activation = x #This sets our mini batch inputs as our first layer activation vector.\n",
    "        activations = [activation] #This creates a matrix to store each of our layer activations.\n",
    "        zs = [] #This creates an array to store all of our weighted input vectors z (or z^l).\n",
    "        \n",
    "        #Feed-forward part of our backpropagation routine.\n",
    "        for b, w in zip(self.biases, self.weights):   #The arrays of both the bias vectors and weight matrices have length num_layers-1 since there are no first layer biases and no final layer weights.\n",
    "            z = w@activation+b   #Calculating our weighted input (for a non-convolutional layer).\n",
    "            activation = sigmoid(z) #Applying the sigmoid activation function to z to obtain our activations.\n",
    "            #Saving each layer's activations and weighted inputs for use in the backward pass later.\n",
    "            activations.append(activation)\n",
    "            zs.append(z)                    \n",
    "            \n",
    "        #Backpropagation part of our backprop routine.\n",
    "        #First we calculate the error in the final layer using our output activations and the derivative of our cost function.\n",
    "        delta = self.cost_derivative(activations[-1], y)*sigmoid_prime(zs[-1]) #Calculates our final layer error vector.\n",
    "        #Next we backpropagate one layer at a time to calculate the error delta for each layer and then we calculate our cost gradient vectors grad_b and grad_w for each layer.\n",
    "        #First we use our last layer delta to calculate the next delta and pass it on: since the last layer is forced to not be convolutional this is just our standard delta calculation.\n",
    "        for l in range(1, self.num_layers): \n",
    "            if l != self.num_layers-1: z = zs[-l-1] #Unless we are on the first layer we will need our previous layer weighted input at each step.\n",
    "            #Using the current layer delta to calculate grad_b and grad_w for a fully connected layer.\n",
    "            grad_b[-l] = delta\n",
    "            grad_w[-l] = delta@activations[-l-1].T   #Again this creates a matrix since we are multiplying our vectors lengthways.\n",
    "            #We then calculate the value of delta to be passed down from this layer to the next. Note that for our last layer we can skip this step as there is no further layer to pass on a delta to.\n",
    "            if l != self.num_layers-1: delta = (self.weights[-l].T@delta)*sigmoid_prime(z)  \n",
    "        return (grad_b, grad_w) #Finally we return a tuple of our cost gradient vectors wrt our biases and matrices wrt our weights for our whole network.\n",
    "    \n",
    "    #Function for evaluating a network's performance on unseen test data.\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    #Derivative of the L^2 cost.\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f30e3106-e5d8-49fa-b5d2-a1edf59e3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function for creating plots of networks' accuracies during training.\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_accuracies(accuracies, color, yrange, single_epoch = False):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    if single_epoch == True:\n",
    "        barwidth = 1500\n",
    "        training_examples_seen = np.arange(0, 50001, 2000)\n",
    "        plt.bar(training_examples_seen, accuracies, barwidth, color=color, edgecolor='black')\n",
    "        plt.xlabel(\"Training examples seen\", fontsize = 14)\n",
    "        plt.xlim([0-barwidth, 50000+barwidth])\n",
    "    else: \n",
    "        epochs = np.arange(0,len(accuracies))\n",
    "        plt.plot(epochs, accuracies, color=color)\n",
    "        plt.xlabel(\"Training epochs (i.e. runs over the training set)\", fontsize = 14)\n",
    "        plt.xlim([0, len(accuracies)-1])\n",
    "        plt.grid()\n",
    "    #Adding a black horizontal line to mark the 10% accuracy obtainable via random chance.\n",
    "    plt.plot([-5000, 55000], [10, 10], 'k', linewidth=1.5)\n",
    "    plt.ylim(yrange)\n",
    "    #Labelling axes and framing plot window.\n",
    "    plt.ylabel(\"% accuracy on unseen test data\", fontsize = 14)\n",
    "    plt.show()\n",
    "    \n",
    "#Creating a function for creating a combined plot of three networks' accuracies.\n",
    "def plot_multi_accuracies(accuracy_array, colors, legend, yrange):\n",
    "    epochs = np.arange(0,len(accuracies1))\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    plt.grid()\n",
    "    for (accuracy, color) in zip(accuracy_array, colors):\n",
    "        plt.plot(epochs, accuracy, color=color)\n",
    "    plt.legend(legend, loc=\"best\", fontsize=12)\n",
    "    #Adding a black horizontal line to mark the 10% accuracy obtainable via random chance.\n",
    "    plt.plot([0, len(accuracy_array[0])-1], [10, 10], 'k', linewidth=1.5)\n",
    "    #Labelling axes and framing plot window.\n",
    "    plt.xlim([0, len(accuracy_array[0])-1])\n",
    "    plt.ylim(yrange)\n",
    "    plt.xlabel(\"Training epochs\", fontsize = 14)\n",
    "    plt.ylabel(\"% accuracy on unseen test data\", fontsize = 14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04ec2a15-0ab6-4417-9404-daa1462e342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the training and test data.\n",
    "training_data, test_data = load_data_wrapper()\n",
    "#Here I also define small and medium versions of the training and test data that can be used for faster but less accurate training.\n",
    "training_data_small = random.sample(training_data, 500)\n",
    "training_data_medium = random.sample(training_data, 5000)\n",
    "test_data_small = random.sample(test_data, 100)\n",
    "test_data_medium = random.sample(test_data, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
