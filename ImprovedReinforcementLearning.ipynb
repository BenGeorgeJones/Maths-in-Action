{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59abc1e5-6040-4491-a03b-ca578f9e1a00",
   "metadata": {},
   "source": [
    "### Improved Reinforcement Learning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d628e-0633-47e8-8e6b-dcff1b02e104",
   "metadata": {
    "tags": []
   },
   "source": [
    "This code contains the Qlearn routine which is used for training neural networks to play games using reinforcement learning.\n",
    "The two games I've implemented with this are Connect 4 and Nought and Crosses which are found in the script GameCode.ipynb.\n",
    "This script also contains routines for pitting two networks against eachother in evaluation games (i.e. after learning) as well as a routine for letting a human play against a network.\n",
    "\n",
    "This is the __improved version__ of the reinforcement learning code that now includes features to improve learning such as:\n",
    "* Experience replay through the use of an experience buffer;\n",
    "* Three exploration types - \"epsilon-greedy\", \"softmax\" and \"adaptive-greedy\";\n",
    "* Data augmentation to generate additional experiences from game-play;\n",
    "* Clipping of rewards;\n",
    "* Infrequent weight updates using the game_per_update parameter to delay experience replay;\n",
    "* Potential for self-play where a network trains against itself;\n",
    "\n",
    "#### Again this code is quite long so feel free to skip straight to Results.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f29b0bec-7acc-4580-b589-00f50d5f7bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function for training two networks against eachother using deep Q-learning.\n",
    "#Here p1 or p2 should be neural networks (i.e. an object from the Network class in the Neural Networks.ipynb).\n",
    "#These could either be newly created networks or pre-existing networks that you want to re-train.\n",
    "#We can also choose to enter the string \"random\" for either player to train the network against a random opponent.\n",
    "\n",
    "#Our other parameters are:\n",
    "#total_games: total number of games to be played during training (this isn't necessarily the number of network updates due to experience replay);\n",
    "#eta: learning rate for networks;\n",
    "#gamma: discounting factor (1 = no discounting, 0 = full discounting);\n",
    "#evaluate: this is a flag that specifies whether to evaluate our networks against a random opponent at every 10% training interval;\n",
    "#lmbda: L2 regularisation hyperparameter for weight regularisation of neural networks;\n",
    "#er_batch_size: mini-batch size used for training via experience replay;\n",
    "#max_buffer_size: number of experiences stored at once in the experience buffer for each network.\n",
    "\n",
    "def Qlearn(total_games, eta, gamma, p1, p2, evaluate = False, save_stats = False, lmbda = 0, games_per_update = 1, er_batch_size = 1, max_buffer_size = 100):\n",
    "    \n",
    "    #First we check that the two networks want to play the same game.\n",
    "    if p1 != \"random\" and p2 != \"random\":  \n",
    "        if p1.game != p2.game: \n",
    "            print(\"Error: Opponents are trying to play different games\")\n",
    "            return\n",
    "        else: game = p1.game\n",
    "    else: \n",
    "        if p1 != \"random\": game = p1.game\n",
    "        else: game = p2.game\n",
    "    (p1_score, p2_score) = (0, 0) #Setting the initial scores to be 0-0.\n",
    "    max_games = total_games  #Saving the number of games to be played overall (as total_games will be decremented later).\n",
    "    standard_reward = 1   #Setting the standard reward/penalty given out during training after networks make winning/losing moves or invalid moves.\n",
    "   \n",
    "    #Preparing vectors for saving stats.\n",
    "    if save_stats == True:\n",
    "        running_scores = np.zeros((total_games,2))\n",
    "        vs_games = []\n",
    "        #We also each network's winrate against a random opponent before any training as a 'control test'.\n",
    "        scores_vs_random = np.zeros((11, 2))\n",
    "        (p1_wins, p1_losses, p1_draws) = NetworkVsNetwork(1000, p1, \"random\", False, False, game)\n",
    "        (p2_losses, p2_wins, p2_draws) = NetworkVsNetwork(1000, \"random\", p2, False, False, game)\n",
    "        scores_vs_random[0] = (p1_wins-p1_losses, p2_wins-p2_losses)\n",
    "        i = 1\n",
    "\n",
    "    #Here we loop over the total number of games to be played in our training session by playing a game to completion (with training) and decrementing total_games until it reaches 0 (at which point we stop training).\n",
    "    while total_games > 0: \n",
    "        board, game_over, current_turn = reset_game(game)  #Resetting the turn number and board for each new game.\n",
    "        (current_player, waiting_player) = (p1, p2)  #Setting p1 to always be the starting player.\n",
    "        #Here we loop over an individual game, during which we get our networks to choose moves and we update the networks according to the observed rewards and values of their chosen moves.\n",
    "        while game_over == False: #As long as the game hasn't been ended by the previous player then the new current player chooses their move.\n",
    "            initial_state = board.flatten()  #Storing and reshaping the initial board state into a vectorised form which is presentable to the current network player.\n",
    "            chosen_action, action_values = select_move(current_player, initial_state, eta, lmbda, max_buffer_size, game, standard_reward)  #Here we get our network to choose a move based on the current board.\n",
    "            initial_scores = (p1_score, p2_score)  #Recording the scores before the next move is made.\n",
    "            board, current_turn, p1_score, p2_score, game_over = input_move(chosen_action, False, game, board, current_turn, p1_score, p2_score)    #Inputs the chosen action and updates the board.\n",
    "            \n",
    "            #If the current player's move has ended the game then we need to update both player's most recent moves with an instantaneous reward/penalty respectively.\n",
    "            if game_over == True: #Also note that since a player can't lose on their own turn, the game must have ended by the current player winning or by a draw.\n",
    "                if (p1_score, p2_score) == initial_scores: reward = 0  #In the case of a draw (i.e. unchanged scores) we give both players a neutral 'reward' of 0.\n",
    "                else: reward = standard_reward #Otherwise the current player must have won so we give them the standard reward (as specified above) and we penalise the waiting player by an equal amount.     \n",
    "                #In the special case of 'self-play' where a network plays itself we multiply any states percieved by player 2 by -1 when saving experiences so that the network always views itself as +1 on the board. \n",
    "                if current_player == waiting_player:\n",
    "                    if current_turn%2 == 0: wp_initial_state = -wp_initial_state  #After an odd turn, player 2 will be the waiting player.\n",
    "                    else: initial_state = -initial_state #After an even turn, player 2 will be the current player.\n",
    "                if current_player != \"random\": save_experiences(current_player, initial_state, chosen_action, reward, None, max_buffer_size, game)   #Here give a positive reward for winning and we set next_state = None to signify that our next state is a terminal state, i.e. the game is over.\n",
    "                if waiting_player != \"random\": save_experiences(waiting_player, wp_initial_state, wp_chosen_action, -reward, None, max_buffer_size, game)   #Here we give a negative reward for losing and set next_state = None to signify that our next state is a terminal state, i.e. the game is over.\n",
    "            \n",
    "            #If the game hasn't ended then the state created by the current player's move can now be used to update the values of the waiting player's chosen move (or specifically to update the waiting player's network).\n",
    "            else:\n",
    "                #We need this state since this is the next actionable state for the waiting player which is used to get the percieved value of this new state as required in the update step for the waiting player's network. \n",
    "                if current_turn != 2 and waiting_player != \"random\":    #We don't perform our network update if our network is the random player or if it's the second turn in the game since then the waiting player hasn't made any moves whose values need to be updated.\n",
    "                    next_state = board.flatten()\n",
    "                    if current_player == waiting_player and current_turn%2 == 0: save_experiences(waiting_player, -wp_initial_state, wp_chosen_action, 0, -next_state, max_buffer_size, game)\n",
    "                    else: save_experiences(waiting_player, wp_initial_state, wp_chosen_action, 0, next_state, max_buffer_size, game)   #Here we now save our next state since the game isn't over and also set our reward to 0 for the same reason.\n",
    "            \n",
    "            #After each turn we swap the active player with the waiting player.\n",
    "            (current_player, waiting_player) = (waiting_player, current_player) \n",
    "            #Saving the most recent player's presented board state and their associated chosen action for use in our next update step.\n",
    "            wp_initial_state = initial_state\n",
    "            wp_chosen_action = chosen_action\n",
    "            if waiting_player != \"random\": wp_action_values = action_values.copy()\n",
    "        #After every 'games_per_update' games we perform experience replay for both networks using their respective experience buffers.\n",
    "        if (total_games-1)%games_per_update == 0: \n",
    "            if p1 != \"random\": experience_replay(p1, er_batch_size, eta, gamma, lmbda, game, standard_reward)\n",
    "            if p2 != \"random\" and p2 != p1: experience_replay(p2, er_batch_size, eta, gamma, lmbda, game, standard_reward)\n",
    "            \n",
    "        if total_games%(max_games*0.1) == 0 or total_games == 1:\n",
    "            #Here we can choose to evaluate our networks against random opponents at each 10% training interval.\n",
    "            if total_games != max_games and evaluate == True:\n",
    "                (p1_wins, p1_losses, p1_draws) = NetworkVsNetwork(1000, p1, \"random\", False, False, game)\n",
    "                (p2_losses, p2_wins, p2_draws) = NetworkVsNetwork(1000, \"random\", p2, False, False, game)\n",
    "                if save_stats == True:\n",
    "                    scores_vs_random[i] = (p1_wins-p1_losses, p2_wins-p2_losses)\n",
    "                    i += 1\n",
    "                    #Here we also save a replay of a game between the networks at each 5% training interval.\n",
    "                    game_replay = NetworkVsNetwork(1, p1, p2, False, True, game)\n",
    "                    vs_games.append(game_replay)\n",
    "                \n",
    "            #Then we print out how many games are left at intervals of 5% of the total games to be played\n",
    "            if total_games != 1: print(\"Games left: \",total_games)    \n",
    "        \n",
    "        #Saving the running scores.\n",
    "        if save_stats == True: running_scores[max_games-total_games] = (p1_score, p2_score)\n",
    "        total_games = total_games - 1  #Decrementing total_games after a game has been played to completion\n",
    "\n",
    "    if save_stats == True: return (vs_games, running_scores, scores_vs_random)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05455c46-9749-480d-a50b-1630db474115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Code for selecting a move during the training of a neural network.\n",
    "#The board state will be fed-forward through the network to produce the networks estimates of each action's action-values.\n",
    "#A move will be chosen based on these values and on the network's exploration/exploitation strategy.\n",
    "def select_move(player, state, eta, lmbda, max_buffer_size, game, standard_reward):\n",
    "    state = state[:, np.newaxis]\n",
    "    if player != \"random\": action_values = player.feedforward(state)   #Here we feed the current board state into our network and at returns the values it estimates each action to have in this state.\n",
    "    if player == \"random\": action_values = None   #If we are playing against a random network then we don't need the action values as no update step is performed on our random opponent.\n",
    "    valid_moves = get_valid_moves(state, game)    #Creates a list of the indices for valid moves to be made, i.e. squares where our board is empty for noughts and crosses or where a column isn't full for connect 4.\n",
    "    \n",
    "    #If we entered \"random\" as our network then our move is always chosen randomly.\n",
    "    if player == \"random\":       \n",
    "        chosen_action = random.choice(valid_moves) \n",
    "        \n",
    "    #For adaptive-greedy exploration the rate at which our network explores depends on the values of the current state.\n",
    "    #We can think of this as a strategy where the more dire the situation is, the more exploration is encouraged.\n",
    "    elif player.exploration_type == \"adaptive-greedy\": \n",
    "        max_action_value = np.max(action_values)\n",
    "        if max_action_value < -1: max_action_value = -1 #Here we clip any action values that go below -1 to prevent numerical instabilities in the next step.\n",
    "        #Here a max action value of 1 results in no exploration, a max action value of -1 guarantees exploration.\n",
    "        if np.random.uniform() > (0.5*(max_action_value+1))**player.exploration_parameter:  #The greater the exploration temperature (i.e. exploration_parameter) the more exploration is encouraged.\n",
    "            chosen_action = random.choice(valid_moves) \n",
    "        else:\n",
    "            #As mentioned above, we usually \"exploit\" and choose the action with the highest value in our state.\n",
    "            chosen_action = np.argmax(action_values)    \n",
    "            #In the case that the network chooses an invalid move we save this experience as a loss. We then allow the game to continue by forcing a valid move to be chosen instead.\n",
    "            while chosen_action not in valid_moves:   \n",
    "                #Here we treat this invalid move as a loss and save it in the network's experience buffer so that the network will be punished for attempting an invalid move during experience replay.\n",
    "                save_experiences(player, state.flatten(), chosen_action, -1, None, max_buffer_size, game) #We give a punishment of -1 for an invalid move.             \n",
    "                action_values_copy = action_values.copy()   #Creating a temporary copy of action values which we will use to ensure that a valid action is chosen next.\n",
    "                for index in valid_moves:   #Having saved this experience as a loss we then ensure that the network picks a valid move so that the game can continue.\n",
    "                    action_values_copy[index] = action_values_copy[index] + 10   #To ensure that the network now picks a valid move we temporarily increase only the valid move values by 10.\n",
    "                chosen_action = np.argmax(action_values_copy)\n",
    "                \n",
    "    #For softmax exploration we choose actions randomly so that higher valued actions are selected more often but all actions have a chance of being chosen.     \n",
    "    elif player.exploration_type == \"softmax\": \n",
    "        chosen_action = softmax_exploration(action_values, player.exploration_parameter) #Here we use exploration_parameter as our temperature coefficient.\n",
    "        #In the case that the network chooses an invalid move we save this experience as a loss. We then allow the game to continue by forcing a valid move to be chosen instead.\n",
    "        while chosen_action not in valid_moves:   \n",
    "            #Here we treat this invalid move as a loss and save it in the network's experience buffer so that the network will be punished for attempting an invalid move during experience replay.\n",
    "            save_experiences(player, state.flatten(), chosen_action, -1, None, max_buffer_size, game) #We give a punishment of -1 for an invalid move.             \n",
    "            action_values_copy = action_values.copy()   #Creating a temporary copy of action values which we will use to ensure that a valid action is chosen next.\n",
    "            for index in valid_moves:   #Having saved this experience as a loss we then ensure that the network picks a valid move so that the game can continue.\n",
    "                action_values_copy[index] = action_values_copy[index] + 10   #To ensure that the network now picks a valid move we temporarily increase only the valid move values by 10.\n",
    "            chosen_action = np.argmax(action_values_copy)\n",
    "            \n",
    "    #For epsilon-greedy exploration we simply explore randomly with probability epsilon each move (here epsilon is denoted by exploration_parameter).\n",
    "    elif player.exploration_type == \"epsilon-greedy\":\n",
    "        if np.random.uniform() < player.exploration_parameter:\n",
    "            chosen_action = random.choice(valid_moves) \n",
    "        else:\n",
    "            #As mentioned above, we usually \"exploit\" and choose the action with the highest value in our state.\n",
    "            chosen_action = np.argmax(action_values) \n",
    "            #In the case that the network chooses an invalid move we save this experience as a loss. We then allow the game to continue by forcing a valid move to be chosen instead.\n",
    "            while chosen_action not in valid_moves:   \n",
    "                #Here we treat this invalid move as a loss and save it in the network's experience buffer so that the network will be punished for attempting an invalid move during experience replay.\n",
    "                save_experiences(player, state.flatten(), chosen_action, -1, None, max_buffer_size, game) #We give a strong punishment of -1 for an invalid move.             \n",
    "                action_values_copy = action_values.copy()   #Creating a temporary copy of action values which we will use to ensure that a valid action is chosen next.\n",
    "                for index in valid_moves:   #Having saved this experience as a loss we then ensure that the network picks a valid move so that the game can continue.\n",
    "                    action_values_copy[index] = action_values_copy[index] + 10   #To ensure that the network now picks a valid move we temporarily increase only the valid move values by 10.\n",
    "                chosen_action = np.argmax(action_values_copy)\n",
    "                \n",
    "    return chosen_action, action_values\n",
    "    \n",
    "#Function for saving the network's gameplay experiences (i.e. initial_state, chosen_action, reward, next_state) to the network's experience buffer.\n",
    "#If our network has data_augmentation enabled then it will also augment these experiences using rotations or reflections to generate new but equivalent experiences to learn from.\n",
    "def save_experiences(player, initial_state, chosen_action, reward, next_state, max_buffer_size, game):\n",
    "    \n",
    "    #Defining a 'flip' matrix used for fast rotations below.\n",
    "    f = np.array([[0, 0, 1],[0, 1, 0],[1, 0, 0]]) \n",
    "    #Here we save the standard experience for the unaltered state; if data_augmentation isn't being used then we are pretty much done.\n",
    "    if next_state is not None: next_state = next_state[:, np.newaxis]\n",
    "    player.experiences.append(tuple([initial_state[:, np.newaxis], chosen_action, reward, next_state])) \n",
    "    \n",
    "    #We can use data augmentation to speed up our learning by also training on geometrically different but still equivalent versions of our actual board. \n",
    "    #For example we can reflect our board horizontally to get a completely different state for our network to train on which is identical as far as gameplay is concerned.\n",
    "    #In each case we save our equivalent boards alongside their equivalent actions, rewards, next states and next greedy actions in an 'equivalent experience' tuple saved to our experience buffer.    \n",
    "    \n",
    "    #For noughts and crosses we can generate at most 7 additional experiences using rotations and horizontal/vertical flips.\n",
    "    if game == \"noughts and crosses\" and player.data_augmentation == True:\n",
    "        #Here we place a 2 in the position of our chosen action which we will use a marker to track where our chosen move gets mapped to as we undergo various transformations.\n",
    "        marked_state = np.reshape(initial_state.copy(), (3,3))\n",
    "        pre_marked_value = initial_state[chosen_action] #Before marking we also make note of what was occupying that space before so that we can undo the marking later.\n",
    "        marked_state[chosen_action//3,chosen_action%3] = 2\n",
    "        if next_state is not None: reshaped_next_state = np.reshape(next_state.copy(), (3,3)) #We also create a reshaped copy of our next state to be used for transforming.\n",
    "        #Generating an experience using the transpose of our unrotated state.\n",
    "        marked_state_transpose = marked_state.T #Taking the transpose of our rotated equivalent (marked) state.\n",
    "        flat_state = marked_state_transpose.flatten()\n",
    "        equiv_action = np.where(flat_state==2)[0][0]\n",
    "        equiv_initial_state = flat_state[:, np.newaxis]\n",
    "        equiv_initial_state[equiv_action] = pre_marked_value\n",
    "        if next_state is not None: equiv_next_state = (reshaped_next_state.T).flatten()[:, np.newaxis] #Taking the transpose of our next state.\n",
    "        else: equiv_next_state = None #If we are in a terminal state then all of our equivalent next states are set to None.\n",
    "        #Saving the equivalent experience to our experience buffer.\n",
    "        player.experiences.append(tuple([equiv_initial_state, equiv_action, reward, equiv_next_state]))\n",
    "        for k in range(3):\n",
    "            #Generating an experience using rotations of our initial state.\n",
    "            marked_state = marked_state.T@f #Rotating our previous equivalent (marked) state.\n",
    "            flat_state = marked_state.flatten()\n",
    "            equiv_action = np.where(flat_state==2)[0][0]\n",
    "            equiv_initial_state = flat_state[:, np.newaxis]\n",
    "            equiv_initial_state[equiv_action] = pre_marked_value\n",
    "            if next_state is not None: \n",
    "                reshaped_next_state = reshaped_next_state.T@f  #Rotating our previous equivalent next state.\n",
    "                equiv_next_state = reshaped_next_state.flatten()[:, np.newaxis]\n",
    "            #Saving the equivalent experience to our experience buffer.\n",
    "            player.experiences.append(tuple([equiv_initial_state, equiv_action, reward, equiv_next_state]))\n",
    "            #Generating experiences using transposed rotations of the our initial state.\n",
    "            marked_state_transpose = marked_state.T #Taking the transpose of our rotated equivalent (marked) state.\n",
    "            flat_state = marked_state_transpose.flatten()\n",
    "            equiv_action = np.where(flat_state==2)[0][0]\n",
    "            equiv_initial_state = flat_state[:, np.newaxis]\n",
    "            equiv_initial_state[equiv_action] = pre_marked_value\n",
    "            if next_state is not None: equiv_next_state = (reshaped_next_state.T).flatten()[:, np.newaxis] #Taking the transpose of our rotated equivalent next state.\n",
    "            #Saving the equivalent experience to our experience buffer.\n",
    "            player.experiences.append(tuple([equiv_initial_state, equiv_action, reward, equiv_next_state])) \n",
    "    \n",
    "    #For Connect 4 we can double our experiences by flipping along the horizontal axis.\n",
    "    if game == \"connect 4\" and player.data_augmentation == True:\n",
    "        reshaped_board = np.reshape(initial_state, (6,7))   #Reshaping our board from a vector into its natural shape so we can perform a reflection on it.\n",
    "        equiv_initial_state = np.fliplr(reshaped_board).flatten()[:, np.newaxis]  #For connect 4 we can get one extra state by flipping our board horizontally.\n",
    "        equiv_action = 6 - chosen_action   #Remapping the initial chosen action to its equivalent action.\n",
    "        if next_state is not None: equiv_next_state = np.fliplr(np.reshape(next_state, (6,7))).flatten()[:, np.newaxis]    #Reshaping the next state into its equivalent state.\n",
    "        else: equiv_next_state = None\n",
    "        player.experiences.append(tuple([equiv_initial_state, equiv_action, reward, equiv_next_state]))   #Saving the equivalent experience to our experience buffer.       \n",
    "    \n",
    "    #Finally we check to see whether we have exceeded the maximum size that we want for our experience buffer and if so we remove some of the oldest experiences.\n",
    "    while len(player.experiences) > max_buffer_size: player.experiences.pop(0)\n",
    "    \n",
    "#Here we determine the vector of target values used in our backpropagation step for reinforcement learning. \n",
    "#The action-values for the unchosen action are typically unchanged unless they exceed the expected range (-1, 1).\n",
    "def get_target_values(network, initial_state, chosen_action, reward, next_state, gamma, game, standard_reward):  \n",
    "    #The only update we potentially do to the unchosen move values is to set their target value to be in the range (-1, 1) if their current value slightly exceeds this limit (usually they will be within this range and this step won't do anything).\n",
    "    initial_values = network.feedforward(initial_state) \n",
    "    #Ensuring that our target values don't go above 1 or below -1.\n",
    "    initial_values = np.where(abs(initial_values)>standard_reward, np.sign(initial_values)*standard_reward, initial_values)  \n",
    "    #If the value of next_state is None then we set the value of the next state to 0 since it's a terminal state.\n",
    "    if next_state is None: next_state_max_value = 0   \n",
    "    #Otherwise we calculate the maximum action value attainable from actions in our next state.\n",
    "    else: \n",
    "        next_state_values = network.feedforward(next_state)\n",
    "        next_state_valid_moves = get_valid_moves(next_state, game)\n",
    "        valid_move_values = []\n",
    "        for index in next_state_valid_moves: valid_move_values.append(next_state_values[index])\n",
    "        next_state_max_value = np.max(valid_move_values)\n",
    "    target_values = initial_values\n",
    "    #Here we set the target value of our chosen action using the RHS of the Bellman optimality equation (for action-values).\n",
    "    target_values[chosen_action] = reward + gamma*next_state_max_value     \n",
    "    return target_values\n",
    "\n",
    "#Code for performing experience replay using the collection of a network's current saved experiences.\n",
    "def experience_replay(network, batch_size, eta, gamma, lmbda, game, standard_reward):\n",
    "    n = len(network.experiences)\n",
    "    random.shuffle(network.experiences) #Shuffling our experiences to be in a random order.\n",
    "    experience_batches = [network.experiences[k:k+batch_size] for k in range(0, n, batch_size)] #Partitioning our experiences into 'experience batches'.\n",
    "    for experience_batch in experience_batches:\n",
    "        mini_batch = [] #Using each experience we will generate training data in the form (initial_state, target_values) which we store in this mini_batch array.\n",
    "        #Loop over each experience in the current experience batch.\n",
    "        for experience in experience_batch: \n",
    "            #Unpacking the experience tuple.\n",
    "            initial_state, chosen_action, reward, next_state = experience   \n",
    "            #Here we use the experience to generate our target values for each action.\n",
    "            target_values = get_target_values(network, initial_state, chosen_action, reward, next_state, gamma, game, standard_reward)\n",
    "            #We can add these to our mini-batch where here our initial state takes the place of our 'input' and the target_values form our 'label'.\n",
    "            mini_batch.append((initial_state, target_values)) \n",
    "        #Using the mini-batch we then train the network using our standard SGD routine.\n",
    "        mb_means, mb_variances = network.mini_batch_gradient_descent(mini_batch, eta, lmbda, n) \n",
    "    \n",
    "#Function for performing softmax exploration when given a set of action values and an exploration temperature.\n",
    "def softmax_exploration(action_values, temperature):\n",
    "    scaled_action_values = action_values/temperature\n",
    "    #Softmax formula.\n",
    "    probabilities = (np.exp(scaled_action_values)/np.sum(np.exp(scaled_action_values))).flatten()\n",
    "    #Choosing an action according to the calculated probabilities.\n",
    "    chosen_action = np.random.choice(range(len(action_values)), p=probabilities)\n",
    "    return chosen_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31be6f85-038c-4467-a492-d1a6efd6e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code that allows two networks to play against each other at either \"noughts and crosses\" or \"connect 4\".\n",
    "#total_games is the number of games to be played and also a random opponent can be specified using the string \"random\".\n",
    "#If show_game is set to True then each turn of the game will be printed out:\n",
    "#Here player 1's moves are represented by +1 on the board and player 2's moves are represented by -1.\n",
    "\n",
    "def NetworkVsNetwork(total_games, p1, p2, show_game, save_game, game):\n",
    "    \n",
    "    #Resetting the scores; these will be kept track of over all the games .\n",
    "    (p1_score, p2_score) = (0, 0)\n",
    "    max_games = total_games\n",
    "    \n",
    "    #Loop over all games to be played.\n",
    "    while total_games > 0:\n",
    "        #Resetting the turn number and board for each new game and setting p1 to always be the starting player.\n",
    "        board, game_over, current_turn = reset_game(game) \n",
    "        (current_player, waiting_player) = (p1, p2)\n",
    "        if save_game == True and max_games == 1: game_replay = [np.copy(board)]\n",
    "        #Game loop.\n",
    "        while game_over == False: #As long as the game hasn't been ended by p2 then p1 chooses their move\n",
    "            state = np.reshape(board, (np.size(board), 1))\n",
    "            valid_moves = get_valid_moves(state, game) \n",
    "            if current_player != \"random\":\n",
    "                #Here we are no longer exploring so we always choose the greedy move.\n",
    "                initial_values = current_player.feedforward(state)\n",
    "                chosen_action = np.argmax(initial_values)\n",
    "                while chosen_action not in valid_moves:\n",
    "                    for index in valid_moves:\n",
    "                        initial_values[index] = initial_values[index] + 10\n",
    "                    chosen_action = np.argmax(initial_values)\n",
    "            else:\n",
    "                chosen_action = random.choice(valid_moves)\n",
    "            #Inputting the network's move to the board (or a random move if using a random opponent).\n",
    "            board, current_turn, p1_score, p2_score, game_over = input_move(chosen_action, show_game, game, board, current_turn, p1_score, p2_score)\n",
    "            #Saving a replay of the game if specified.\n",
    "            if save_game == True and max_games == 1: game_replay.append(np.copy(board))\n",
    "            #After each turn we swap the active player with the waiting player and continue the game.\n",
    "            (current_player, waiting_player) = (waiting_player, current_player)      \n",
    "        total_games = total_games-1\n",
    "    \n",
    "    draws = max_games-p1_score-p2_score\n",
    "    if show_game == True:\n",
    "        print(\"\")\n",
    "        print(\"Final scores (p1, p2): (\",p1_score,\"-\",p2_score,\")\")\n",
    "        print(\"Draws = \",draws)\n",
    "        print(\"\")\n",
    "    if save_game == True and max_games == 1: return game_replay\n",
    "    return (p1_score, p2_score, draws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "590fde83-d856-415b-9ca9-57d64d8589b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Code that allows a human to play against a neural network at either \"noughts and crosses\" or \"connect 4\".\n",
    "#Here player 1's moves are represented by +1 on the board and player 2's moves are represented by -1.\n",
    "\n",
    "def HumanVsNetwork(p1, p2, game):\n",
    "    \n",
    "    #Resetting the turn number and board for each new game and setting p1 to always be the starting player.\n",
    "    board, game_over, current_turn = reset_game(game)  \n",
    "    (current_player, waiting_player) = (p1, p2)\n",
    "    p1_score, p2_score = 0, 0\n",
    "    #Game loop; as long as the game hasn't been ended by the previous player then the new current player chooses their move.\n",
    "    while game_over == False:\n",
    "        #Reshaping the board so it's ready to be printed for the player to see.\n",
    "        if game == \"noughts and crosses\":\n",
    "            print_board = np.reshape(board, (3,3))\n",
    "        if game == \"connect 4\":\n",
    "            print_board = np.reshape(board, (6,7))\n",
    "        valid_moves = get_valid_moves(board, game)\n",
    "        \n",
    "        #Human's turn.\n",
    "        if current_player == \"human\":\n",
    "            print(\"Your turn: \\n\",print_board)\n",
    "            if game == \"noughts and crosses\":\n",
    "                #Making sure that the chosen move is valid.\n",
    "                human_move = int(input(\"Choose a square: \"))\n",
    "                while human_move not in [1,2,3,4,5,6,7,8,9]:\n",
    "                    human_move = int(input(\"Invalid move, please choose a number from 1 to 9: \"))\n",
    "                while human_move-1 not in valid_moves:\n",
    "                    human_move = int(input(\"Chosen square is full, please choose another square: \"))\n",
    "            if game == \"connect 4\":\n",
    "                #Making sure that the chosen move is valid.\n",
    "                human_move = int(input(\"Choose a column: \"))\n",
    "                while human_move not in [1,2,3,4,5,6,7]:\n",
    "                    human_move = int(input(\"Invalid move, please choose a number from 1 to 7: \"))\n",
    "                while human_move-1 not in valid_moves:\n",
    "                    human_move = int(input(\"Chosen column is full, please choose another column: \"))\n",
    "            #Since the actual indices start from 0 we have to remove 1 from the selected moves.\n",
    "            chosen_move = human_move - 1\n",
    "            \n",
    "        #Network's turn.\n",
    "        else:\n",
    "            print(\"Network's turn \\n\",print_board)\n",
    "            state = np.reshape(board, (np.size(board), 1))\n",
    "            move_values = current_player.feedforward(state)\n",
    "            chosen_move = np.argmax(move_values)\n",
    "            while chosen_move not in valid_moves:\n",
    "                for index in valid_moves:\n",
    "                    move_values[index] = move_values[index] + 10\n",
    "                chosen_move = np.argmax(move_values)\n",
    "            if game == \"noughts and crosses\":\n",
    "                print(\"Network chose square \",chosen_move+1)\n",
    "            if game == \"connect 4\":\n",
    "                print(\"Network chose column \",chosen_move+1)\n",
    "                \n",
    "        #Inputting either the human's or the network's move to the board.\n",
    "        board, current_turn, p1_score, p2_score, game_over = input_move(chosen_move, False, game, board, current_turn, p1_score, p2_score)\n",
    "        #After each turn we swap the active player with the waiting player and continue the game.\n",
    "        (current_player, waiting_player) = (waiting_player, current_player)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0927d-6cb6-4296-9b73-6687904770fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for plotting win rates against a random opponent obtained during training.\n",
    "def plot_stats(stats):\n",
    "    #Unpacking the stats tuple.\n",
    "    (vs_games, running_scores, scores_vs_random) = stats\n",
    "    total_games = len(running_scores)\n",
    "    \n",
    "    #Plotting of the running scores of each network during all training games.\n",
    "    games_played = np.arange(1,total_games+1)\n",
    "    #Plotting player 1 in red and player 2 in blue.\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(games_played, running_scores[:,0], color='red')\n",
    "    plt.plot(games_played, running_scores[:,1], color='blue')\n",
    "    #Labelling axes and framing plot window.\n",
    "    plt.xlim([0, plt.xlim()[1]])\n",
    "    plt.xticks(np.arange(0,total_games+1, 0.1*total_games))\n",
    "    plt.ylim([0,plt.ylim()[1]])\n",
    "    plt.xlabel(\"Training games played\", fontsize=14)\n",
    "    plt.ylabel(\"Scores during training\",fontsize=14)\n",
    "    plt.legend([\"Player 1 Score\", \"Player 2 Score\"], loc=\"upper left\", fontsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "    #Plotting the winrate of each player against a random opponent at 10% training intervals.\n",
    "    games_played = np.arange(0,total_games+1, 0.1*total_games)\n",
    "    bar_width = 0.03*total_games\n",
    "    bar_offset = 0.015*total_games\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(games_played-bar_offset, scores_vs_random[:,0]/1000, bar_width, color='red', edgecolor = 'black') #Plotting player 1 in red.\n",
    "    plt.bar(games_played+bar_offset, scores_vs_random[:,1]/1000, bar_width, color='blue', edgecolor = 'black') #Plotting player 2 in blue.\n",
    "    plt.legend([\"Player 1\", \"Player 2\"], loc=\"upper left\", fontsize=10)\n",
    "    plt.plot([-2*bar_width, total_games+2*bar_width], [0, 0], 'k', linewidth=0.9)\n",
    "    #Labelling axes and framing plot window.\n",
    "    plt.xlim([-2*bar_width, total_games+2*bar_width])\n",
    "    plt.xticks(games_played)\n",
    "    plt.ylim([plt.ylim()[0], 1])\n",
    "    plt.xlabel(\"Training games played\", fontsize = 14)\n",
    "    plt.ylabel(\"Average score against random opponent\", fontsize = 12)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
